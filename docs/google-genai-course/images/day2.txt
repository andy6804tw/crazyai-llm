DAY 2 Livestream - 5-Day Gen AI Intensive Course | Kaggle - YouTube
https://www.youtube.com/watch?v=AjpjCHdIINU

Transcript:
(00:01) awesome welcome back to the second day of our kaggle generative AI intensive course we're really excited to have you over 250,000 developers worldwide here to learn today all about AI about the Gemini models about AI Studio about uh vertex Ai and all of the other great products that Google has been releasing over the last several months um we're Overjoyed to to see the responses in YouTube Discord and all of the places and we can't wait to tell you today about embeddings and Vector databases um so just as a quick reminder our course
(00:35) overview um this is a 5-day generative AI intensive course brought to you by the kaggle team in collaboration with Google cloud and Google deepmind um it is um a kind of a very quick round trip through all of the all of the things that you might have heard about in the generative AI space and today we are on day two embeddings and Vector databases if you would like to learn more about foundational models and prompt engineering please refer to all of the content that we shared yesterday um as well as the YouTube recording you should
(01:05) be able to access both of those um through all of the all of the content that we've that we've shared on kaggle um my name is paig Bailey I lead developer relations engineering at Google deepmind um as a quick reminder the content that you should have already been referring to are the the P uh the summary podcast episode which was coincidentally generated by notebook LM um we'll be talking to the notebook LM team a little bit later this week the embeddings and Vector stores databases white paper um as well as a few code
(01:36) Labs on kaggle that are not we'll be going through later in this section and um to give a quick overview of the white papers I'm now going to uh hand it over to an not um after quickly thanking all of our wonderful moderators if you see them on Discord they are working tirelessly around the clock to answer your questions um to make sure that you have everything that you need in order to be successful for the course so please uh please give them uh a lot of emoji love um and say thank you um I especially want to call out Brenda Flynn
(02:08) um as well as kenal um they have been kind of doing all of the production and Stage management behind the scenes and they're really making this course happens so thank you so much to all of our wonderful moderators um and with that uh anant do you want to take over the quick curriculum overview sure thing Paige hi everyone um uh it's me again to give you an overview of what we are learning together or what we learned today in today's white papers to kind of set the stage for the Q&A session which a lot of good questions uh you've asked
(02:39) as well so um today's white paper day two's white paper was about embeddings and Vector databases and roughly um we started off by seeing uh with learning about embeddings what embeddings are how they're useful and uh we learned that they are basically numerical Vector representations of data let's say text images and more they map diverse data into common semantic space where distance can be used as a proxy for semantic similarity now this is great for comparing different pieces of data or even the same pieces of data
(03:16) efficiently or same or similar pieces of data efficiently and they can also be used as a rich semantic representation for Downstream models tasks and applications we also looked at the various types of embeddings such as text embeddings which evolve from early methods like word to W all the way to context aware models like bir and the Gemini backbone uh based embeddings uh we also have uh image and multimodal embeddings plus also there ways to embed IM uh structured data and graphs into their uh semantic representations then we looked at how
(03:53) training for embeddings often uses dual encoders and contrastive laws to group similar items this is one method to train embeddings there's also others and then evaluating the quality of embeddings uh using metrics like precision and recall as well as um others on standed benchmarks is quite important when doing so we then looked at how we can search through billions of these vectors quickly so hint it's not with linear search right we uh we also saw how we can use Vector search with approximate nearest near neighbor
(04:29) algorithm Ms uh some of them which we looked into the uh which we read about in the paper uh include scan as well as uh hnsw which are um two very popular algorithms and these basically these a&n algorithms trade a tiny bit of accuracy for massive speed gains uh and then we also saw how these algorithms are leveraged by Vector databases which are built specifically to store manage and query these embeddings efficiently at scale we then concluded the white paper by talking about applications of all of these Technologies and we looked at
(05:06) various things like how embeddings can be used for search recommendations for um RG or retrieval augmented generation uh based workflows together with llms and um we can also um uh we we also saw how um the the embedding part provides the context used in the retrieval stage of the RG pipeline to generate more factual and grounded answers so these things are powerful and I'm uh we'll cover more in the code lab I'm heading it over to paig for the Q&A session take you to Pig yep absolutely I I think that many people have had um a strong success
(05:46) using Vector DBS and embeddings and uh setting up retrieval systems especially for for building chat Bots externally so it's it's great to give people the tools in their toolkit in order to do all of this successfully um and so with that I am going to head into the Q&A I am so excited and Overjoyed today to to Welcome to the stage um many folks from um from Google Cloud's Office of the CTO as well as from Google deepmind um I at jazz today to have Patricia Allan shiai um Howard Andre Chuck um and an aunt to
(06:21) to talk through some of the questions that y'all have asked as well as to give insight into all of the awesome things that they're building with embeddings and Vector databases both in research and out in the external World um so first question is for Andre and for Howard um creating effective embeddings is crucial as we saw in many of the things that we were reading uh over this past 24 hours what are the latest advancements or best practices in generating embeddings that capture nuanced semantic meaning especially for
(06:51) specialized domains or multimodal data which are increasingly becoming important um uh being able to embed things like images or videos um how much does the choice of the embedding model impact the performance of the downstream Vector search task and Andre do you want to start yes yes thanks paage hi everyone thanks for the question so to start with the advancements a notable one recently is should integrate llms in the embedding model development stage as I guess you would expect so llms can be used as a pre-train back bomb to initialize the
(07:25) embedding model and this allows the embedding model to already leverage multilingual and multim model understanding and not necessarily have the have the Ed model uh need to learn multiple languages or multiple modalities directly at the embedding training stage and the second point about llm usage is that it can help refine training data sets with data curation or by generating high quality training examples so I encourage you to take a look at the recent Gemini embedding model and the associated tech report which discusses these points in
(07:56) detail now to talk about mul model specifically talking about embeddings which understand images spatial awareness is a property that's often lacking so for this reason recently my team at Google deep mine introduced a powerful new embedding to address this issue called tips and this stands for text image retraining with faal awareness this a paper that we'll present later this month at I clear which is one of the main AI conferences so I encourage you to take a look at that too we have code and models publicly available for this model and to
(08:29) the final point in the question about how eding quality can impact performance this can be very significant actually so for many applications on retri manager generation or rag just like anut was uh mentioning uh you know having an LM model uh uh by itself it would simply not be able to answer questions if the uh right documents are not found so this is of course a little bit application dependent for example may depend on the size of the index you may be retrieving from or if types of data that are being retrieved are similar to what the
(09:02) embedding model was trained on so um I encourage you to carefully consider embedding Choice as it could definitely be a bottleneck for AI systems today especially given that L are already so powerful okay thank you absolutely and we'll be adding the links to the papers that you mentioned as well as congratulations on getting accepted for I clear that's super exciting um I and if folks are interested in reading more about uh about Andre and his team's work um that would be a great place to learn more Howard is there anything else that
(09:35) you would like to add uh thank you Paige and thank you Andre um just a couple I think Andre covered it um pretty well uh just want to add a couple of U more points uh one is uh there's this um technique called the matrica uh embedding um which is actually currently pretty popular the idea is that if you actually have an embedding space was let's say a dimension of a thousand um we're training uh put putting some training uh uh you know uh requirement here to train embedding which uh uh require the um shorter length embedding for instance uh
(10:16) the first 200 um Floats or the first you know 100 floats to be also in in the same embedding space in that uh you can basically uh trim the embedding uh such that you still perform similarity uh similarity uh measure uh and they're all valid and that way um you can um you can you know decide how how big of the embedding you want to store from one training um and then you can always um well each Dimension does require uh you know pre to be pre specified but this is a pretty powerful technique for uh being able to uh efficiently represent
(10:56) embedding in these type of vectors store uh database scenario um the other thing I I do want to I would like to emphasize is the use of a uh for data curation uh this happened in uh you know many recent papers including the Gemini embedding paper uh LMS are really powerful and we all know that and then uh it is you know crucial uh right now it provides this um you know um capability for filtering lowquality examples and determining relevant positive and negative passages uh for retrieval and generate really
(11:34) rich synthetic data set um and then uh one last thing I want to mention is that uh you right now some of the papers like ev5 uh they actually use um multimodel RMS uh to bridge the gap uh for input so this is mostly um used for multimodel embedding um and I think um you will see that Trend uh become more uh relevant uh in we in in in Google uh we're also working on something like that and then I think you will see um um Gemini embedding uh to become multimod soon yeah awesome thank you both for pushing the boundaries of what's possible with
(12:14) embeddings and making sure that we can get these things out into the world so others can make use of them not just not just Google and not just deepmind this is really really cool and one of the um one of the most powerful ways to get research out into the world um next question um for Allan uh so when integrating Vector search capabilities into existing database systems like alloy DB versus using dedicated Vector databases what are the primary architectural tradeoffs regarding performance so things like latency and
(12:44) throughput cost data consistency and ease of use for developers who are building applications um like rag or semantic search and also for folks in the audience who might not have heard of alloy DB before um if you could just give like a couple of sentence description of what alloy Tob is uh and why it's useful sure thanks Paige and thank you for the question um so my name is Alan Lee and I'm part of Google uh Cloud databases and the lead for allo DB semantic search uh so allo DB is uh one of gcp databases um database offering uh
(13:21) it uh it is uh fully um postgress compatible uh with additional Innovations on the architectural side to make it uh faster than what is available for open- Source uh postgress and uh this uh allows our customers to be able to take uh their operational database workloads and um and run it faster on Allo DB and uh We've Incorporated um Vector um search capabilities in it uh as uh additional um uh advancements uh for our offerings for customers and so um going back to your question um so while performance is listed uh first in
(14:10) the question and is often the top of Mind metric that U many think about I do see more and more non-performance requirements from talking with various customers and part of the shift is uh coming from the improvements in Vector search uh performance of existing database systems that uh will generally work well for 90% of the use cases and so um as an example alloy DB um it's uh We've Incorporated um the scan algorithm as a native Vector index type and um as uh the uh course participants uh may know from the white paper uh scan is
(14:56) based on 12 plus years of Google research and is our state-of-the-art Vector search algorithm that's used internally at Google scale for products like Google search YouTube ads and um the this algorithm has given alloy DB the vector search performance that is more performant and scalable than what's available in open source and it's also competitive with specialized Vector uh uh databases is uh out there and um and so other important architectural consideration is that um uh we should all be thinking about when um building on top of vector search
(15:44) is where the other data like what other data needs to be accessed for the overall application and where does that uh data come from so for example filter search is a very common use case for many customers and filter search combines a vector search query for example shirts that look like this uh image embedding and filtering conditions um size equals small and color equals purple and price less than $50 but where where this uh this do these other filtering condition data reside is it um structured data that's
(16:25) already in an operational database or is it uh in a data Lakehouse like bigquery so if that's the case uh it may be easier to leverage uh the vector search capability of those systems and uh that will allow the uh user to express their Vector search along with filters and joints and uh in a single system and in a single interface like SQL um otherwise uh there will be additional operational overhead and cost of things like ETL pipelines and implications of dealing with data consistency issues across systems and
(17:07) application complexity of having to join across multiple systems and uh that's without the help of a query Optimizer that's typically there for SQL based systems and these things make it harder for developers um specialized Vector databases like our Vector search uh vertex Vector search have their place as well um if data is largely unstructured like video images and documents that don't already typically reside in a database or a data warehouse then you know none of the data consistency or application uh complexities that I
(17:44) mentioned apply and specialized Vector databases can work quite well in these cases and um unlike databases and uh warehouses um that have to generalize across various workloads including non Vector workloads uh specialized systems can squeeze out the last bits of performance and that may matter for the 10% use case and uh usually this comes with tradeoffs like higher cost for purely serving out of memory or having looser data consistency models for index rights and updates and lastly depending on the use case some other important
(18:26) considerations include security compliance reliability mechanisms like backup uh ha features just generally speaking existing databases uh have more mature enterpr Enterprise grade features uh than specialized systems and that's mostly as a function of how long they've been around so um uh so it's important for the practitioners to think about like all the requirements of their entire application that makes a lot of sense and I love that we've been able to take kind of this decade plus investment and research
(19:05) at Google and get it out into the world into the hands of customers through alloy DB um that's I I I'm hoping to uh to learn more about some of the use cases um at the Google Cloud next as well I I know that many customers have have been implementing aloy DB in production and then uh getting serious gains from it um so thank you for the great uh thank you for the great answer next question um for Patricia and I I believe also uh also Alan might have some some thoughts around this too um from a strategic Viewpoint how do you
(19:40) see the rapidly evolving landscape of vector embeddings and databases impacting interprise data architectures and the types of AI driven applications that businesses can build what are the key challenges so things like cost management standardization Talent gaps um that organizations face in adopting these Technologies at scale and in your and your role in the office of the CTO I'm sure you must see this for many Enterprise customers yes fantastic um thank you for the question because this is a very important one I think that traditionally
(20:12) we have often underestimated the depth and breadth of the impact that certain Technologies may cause and I think that we are in the middle I don't like to call it a paradigm shift but to offer or introduce an alternative where we are going from optimizing the indexing and retrieval capabilities of our data architecture to an exact match like in SQL across several fields of your structure Data Corpus to being able to create several indexes of your data across multiple Dimensions to optimize how you index on a per task bases you
(20:53) can create multiple indexes per database and to search and retrieve data based on on some fit for purpose metrics such as semantic similarity for search for example or proximity uh for certain attributes in in recommendation systems and all of this in multimodel data where one can be looking for similarities but much more like nuances between video audio streams text images not just between concrete data points on the same modality as we are so used to do with equal today and this requires a paradigm shift across several Dimensions so if
(21:34) you look from an architecture perspective we have to really learn or relearn how to ingest organize manage make accessible new modalities of data that we have not dealt with on a daily basis as we are doing now especially s and on structure um data this also includes learning the variety of indexing mechanism especially the ones that Google cloud and Google offers available to the different storage types and also understand the trade-offs between accuracy performance and scalability which I know it was in the
(22:13) white paper that you read also uh from a development perspective it demands developers to have like a deeper understanding of the meaning of embeddings which is a non-trivial concept as we also learned uh in the last day and uh from an application experience perspective we really to need to reimagine the user experience that can now be offered because you can retrieve such a complex set of uh multimodality data uh but it doesn't come without its challenges uh you have cost the infrastructure cost is associated with
(22:51) deploying and maintaining Vector databases standardization as you saw there is a lack of universal industry standard for vector embeddings and database interfaces poses integration and interoperability challenges I would say the talent Gap uh but now we have another 250,000 people that are familiar with um uh the concept of embeddings and Vector databases there are issues of scalability of and performance because you're really handling massive data sets with billions of vectors while you want to maintain low query latency and a
(23:31) careful architecture design and selection of indexing techniques and you add on top of that data governance security and compliance that cannot be neglected and integration with existing systems integrating Vector databases with traditional Rel relational databases and existing business applications which can be very very complex however we are very very very very optimistic because uh multimodel data representation through their embeddings in a scalable performance and accurately retrievable manner we believe
(24:10) that is going to form the foundation of modern Enterprises architecture so thank you for the question absolutely I I know that um you know increasingly things like video understanding being able to to have um insight into all of these different modalities are super super important and I love that you called out that thanks to all of the folks who are dialing in we now have over a quarter of a million developers out into the world who who will be able to help with some of these Mission critical tasks going forward so this is this is great to know
(24:43) um next question um this is for Chuck uh Beyond standard cosign similarity or ukan distance what more sophisticated techniques or indexing strategies so things like incorporating metadata filtering um or hybrid search approaches reranking um are proving the most effective for improving the relevance and accuracy of vector search results in real world applications and how are we uh sort of taking all of these learnings and embedding them into platforms like vertex AI um or or in Google Cloud great question um thanks for the
(25:20) question and thanks for all the questions on Discord like we'll try and get to them offline if yours didn't get ented here but uh lots of great activities there so thank you um I'd say just to start off and frame that question there's a lot there about as you as you've seen in that question there's a lot of knobs you can turn for embedding systems it can make a big difference especially when you're doing something at scale but they might not be necessary for your use case and so you know you heard Allan talk about the 90%
(25:43) use case the 10% use case 1% use case and so you're anybody who's talked to me is going to hear me talk about the importance of like a a good evaluation Suite so if you have an example Benchmark data set or you know something that can help you figure out whether or not it's going to make a difference for your use case to add that extra complexity um in practice what we see people doing or being successful with um out of the things you were talking about I would say rag search a rag hybrid search is one that really makes a big
(26:09) difference so for people aren't familiar this is the idea of taking what we're talking about here with the embeddings those semantic embeddings usually um and then combining them with traditional keyword matching so you want to be able to get it when somebody's when you have something that isn't exactly the same but is related so a synonym you're going to be able to pick those up with embeddings it really helps makes a big difference but those traditional keyword systems where if someone actually knows the title of the article they're looking for
(26:33) or the idea of the part that they want to find or the product you really want to support that too and so combining those two things and what people call hybrid search really helps I think it's going to get more complicated um as you go on because we have people like Howard and um we have people like Howard and Andre who are doing amazing work with multimodal and so now hybrid search is going to start to include things like video and uh images and other things so it will get a little bit more hybrid so just be aware for that
(26:59) um and we you can see that in our tools so like at Google cloud and vertex AI Vector search we support both spars which would be those keyword embeddings or keywords and then semantic in the same in the same tool but you can also just pick up uh like bm25 in popular python package you can just try that out so like if you look at code snip at zero and the white or Cod snip at one in the white paper you could just drop in bm25 and do the same thing and get the same measurements for precision and recall and get a feel for how that would work
(27:27) for you um when you start to add additional uh additional modalities so like if you're going to do keywords or you're going to do images then you're going to fall into you're gonna probably want to do some kind of reranking at the end you may want to do reranking anyways but it's really reranking is really helpful for finalizing that customer that final list to the to the customer or person who's going to be reading it um and so you know some maybe you want to like combine a few different retrieval methods you
(27:54) could just look at the ranks but or maybe you want to do something where you're actually going to rank certain is higher so like a an official frequently asked questions document might be more reliable than something that you a document that you pull off of like the community notes and so you'll see people do that and it can be really helpful for those sorts of use cases um just keep in mind your latency budget whenever you're doing that sort of stuff some of the other things that were in the question like I would sort of qualify as ease of
(28:20) use and really make a big difference at scale so um you know filtering something can really cut down on latency if you can get rid of most of the relevant answers irrelevant documents early in your process um the same thing I would say for streaming inserts if you don't want to like rebuild your index all the time that can be really helpful for your team I think anytime you're picking up a new Vector database you really want to make sure that that it's going to meet your your company and your organization's ability to support it and
(28:44) maintain it like is it gonna is it gonna be easy to use is it goingon to be easy to monitor it does it meet your privacy and security requirements so it's you know it is a new piece of infrastructure and you just want to be in sync with your other stakeholders um and make sure it's something that your group can can own and maintain and scale because I you know like like people were saying it this really is a key uh piece of infrastructure at Google and I think it probably will be at your at your organization or in your projects as well
(29:08) going forward so I think that covers it all there are a lot of knobs to turn uh ANS please feel free to ask more questions or follow-ups on the on the Discord and we'll be hanging around there to try and answer things their time zones permitting thank you awesome I love that answer and I love how you're making it real for people who might be hoping to implement some of these things in production things like filtering and reranking are are kind of hard-learned lessons and uh and optimization approaches internally at Google but I I
(29:34) think for folks who are just starting on their Journey those are um those are good things to call out and to have top of mind on day one so love that answer um next question also for Chuck um for for embedding models this is from uh uh from our Discord Cyclone um Cyclone dester uh which I which I wonder is like Cyclone uh cyclone clone Destroyer uh I'm not sure how to pronounce it but but I'm going to try my best for embedding models how do I upgrade to the newer embedding models um when they're available do we need to run all of the
(30:09) data in the database again through the new embedding models or is there a better way to do it um so it sounds like this person is just getting started with embeddings and they're they're trying to understand all of these different models that are available um how might they how might they use them effectively so I hate to be the bear of bad news but you do have to upgrade all of your embeddings that you've done when you switch to a new model um and that's that's one of those things embedding models just aren't compatible with each
(30:37) other and so that's like that's just sort of the the way the world works I I apologize but it is like different languages or uh you know different versions trying to link to a to a different binary library that was compiled with something else try you know um so that that is the way it is I think you know the the thing I'm going to say again is just the importance of good evaluations so when you're doing that upgrade you want to be able to know that you have the matching version for the for whatever query you're using and
(31:04) whatever the indexes that you're querying and so having a good evaluation Suite really helps that and I would say in addition to the usual precision and recall and sort of like AI based metrics that give you a sense of the quality of those uh of what you're retrieving don't forget to be measuring things like latency and other like important system metrics like it you know the throughput the latency how does it perform under load because you will see what these more sophisticated models models they do have a lot more parameters and so maybe
(31:31) they need a little bit more Hardware um but I think and there's nothing like qualitative feedback from your users but but having a good like even a not so good evaluation Suite can really help smooth that process and make sure that that everything is working um and for you and your users as you as you change these embedding models but yes you do have to regenerate the embedding model the the embeddings for your index if you upgrade the embedding model um and I would say the way things are moving you've got smart people at Deep Mind and
(31:59) and in the community doing amazing work and you're going to want to be able to pick up those changes because it really they really are big improvements and so I would say just be ready for uh for iteration it's a it's the train is moving quick enjoy it yep and the I I love that I love that quote the train is moving very quickly we just released a new embedding model on AI studio just a a few a few weeks ago um Howard it also uh did you want to add something yes um and thank you Paige and thank chck for uh you know um endorsing our working at
(32:32) Jeep deep mine I just want to add a little bit one one thing that U we're thinking you know leave no one behind so the train is leaving but we also try to get the custom uh you know get the uh the passengers on this train uh so this is active research area um but I think what we're really looking at is um how do you think about the embeddings the previous embeddings and the newer embedding edings and is there a more efficient way to map from previous embedding to the newer embeddings um I I think in terms of ongoing research we
(33:06) see fairly promising results it's not deployed it's um I think um probably um many people in the research Community uh are looking at the same problem because it is a pain to upgrade the entire database to new embeddings especially when the new edings are coming every day right so just want to um add that this is an ongoing research area we see very promising results and hopefully we'll see that in uh in Google cloud in many places soon awesome that's great to hear and awesome to know that uh that folks in the research world are are kind of
(33:40) pioneering some of these approaches to make life easier for all of the developers out there so appreciate appreciate your response to this question too next question um in retrieval augmented generation so ra these rag systems what potential impact could arise in retrieval accuracy or the quality of llm generated responses uh if different embedding models are used for documents and queries um so shiai do you want to do you w to answer this one yes uh so s page uh hi uh I'm sh I'm from Vortex I'm the uh like techic manager on
(34:18) the vortex embedding quality so uh regarding to this question it's very critical that we use the same embedding model for document embedding and the cor embedding generation uh the reason behind that is the embeding models they transform complex data like text image audio videos into a shared Vector spaces however this specific mapping uh from like real objects to Vector spaces are different model by model so using the same embeding model in R will ensure that both queries and documents are projected into the consistent embeding space this
(34:57) is crucial because the distances between the shared spaces are then used to accurately reflect the semantic Rel relationship between the queries and documents so please try to use the same uh like queries and documents from the same eddings uh when you build your own work excellent thank you for highlighting those best practices and I I think as many people are just starting on their Journeys uh those are the the kind of gold nuggets that make all the difference uh to make sure that they get started on the right uh on the right
(35:28) foot and in the right place awesome um next question um in a rag system indexing is performed in advance on the application data set so how does the system ensure that query results remain relevant to the current context in a dynamic or real-time application after deployment I love this question about kind of system drift um Patricia an not do you want to do you want to take a stab at the answer should may we start um there so first of all uh uh yes me measuring drift to see how embeddings um the change over time is
(36:05) Super critical and coming back to what Chuck mentioned having an evaluation suit but also uh to measure the drift in performance but also General Distribution D drift is important however there's one more key point which I think um especially in the context of RGS or how embeddings are used with retrieval augmented generation systems which can make this whole process more efficient is that embed the process of embedding as well as embedding entire databases also depends on the dimensionality of the embeddings uh um
(36:35) so if if you have a larger embedding of say 2,000 um um however we if you use if you consider the concept of matushka embeddings where we can trim the embedding down and also consider the fact that um um like uh like you can make the the whole process much more uh like efficient because since uh in larger llms with increasing context Windows you can retrieve a lot more than just the top 10 or top five documents so you do not need to necessarily care so much about the performance of the top uh like hits at K the the retrieval
(37:13) performance in the top five tops items you can afford to uh focus more on like uh recall rather than Precision where a few false positives are okay with our increasing context windows for example G feminize um I think 2 million one to two million context window depending on the model you use is allows us to use slightly less performant embeddings but allow the indexing process to be much more efficient because we can use embeddings of lower dimensionality so um that's one aspect to it I believe um Patricia and Alan had things to share on
(37:49) this absolutely no thank you uh for that because this is really uh an brought up that you show of using the TT window and the flexibility on the retriever allowing for false positives that can actually give a little bit more of uh items to be uh retrieved or exposed to the user uh on the other side uh of the equation you would say on a complementary side of the equation what you're trying to do is effectively Bridging the Gap between static index and involving data as well so another aspect is to say look the data has drift that we indexed the
(38:31) world as we saw it some time back and now the world is different there are items that have been added and items that have been removed and there are several strategies focused on updating the vector database and its index in near real time and response to data changes so one technique is called incremental indexing and which allows for the addition of new data to the index without re requiring a complete rebuild is like adding these information and reduces the latency and computational cost that is associated
(39:06) with keeping the index always Cent there is always uh also um what is called hnsw which is the hierarchical navigable small world which is a popular indexing structure that generally handle the insertion of new vectors more efficiently than three based the indexes like anoi um research continues to advance in this field and we should always be mindful and uh look at what is coming one of them is called the advanced uh inverted file which achieves higher um updated throughput I would say uh by maintaining policies and local re
(39:48) reclustering but this is not all because of vector data basis they often employ inmemory buffers to handle new data and background prod processes as they merg these changes into the main index so change data capture mechanisms actually provide an automated way to trigger updates to the vector database whenever changes occur in the source application data set so we have to look into that as well continuous indexing and embedding generation can also uh consume significant computation of resources so we really need to be mindful of that an
(40:26) alternative approach just to wrap up is the real time rag or no index rag so you don't create the index at all it completely bypasses the need for pre-computed indexes by directly quering live data sources at the time of the user query in this architecture when a user actually submits a question the system transforms the query into a format suitable for the source systems native search API and uh uh submits this transform the query to the Rel adved data sources um in any case It's always important to actually achieve a tradeoff
(41:06) between cost performance um accuracy scalability and freshness of the data I love that answer and Ellen do you want to add something in addition yeah just to uh briefly to add on to Patricia's answer I think an important part of uh operational databases is the transaction and strong consistency guarantees uh between reads and writes and that includes uh index updates and um so uh What U you know the transactional semantics uh means is that inserts and updates that are successfully written or uh immediately visible for subsequent reads and uh this
(41:51) helps developers avoid dealing with eventually consistency issues like having staled data that shows up uh uh that has to now be hidden from the user um and this is also important for some business CA use cases where it is important to like have visibility to the latest data so again this goes back to you know considering the um the requirements of dentar application um and in addition to what anant said uh about um you know the data drif issues um you know there is also Innovative work on U automatically handling index
(42:34) uh distribution or drift issues U to make it easier to use for um for everyone awesome thank you for the great question pant I as a workaround for this I I sometimes use the the Google search API feature within the the Gemini apis um so you can ground information on the latest Google search results which sounds very similar to one of the things that Patricia was mentioning um of just using kind of a a search to query the the live data source and and to get the response back and to incorporate that into into a summary um so lots of tools
(43:11) in the toolkit to make sure that as you're building your outputs from uh the large language models um or building these systems which Drive the outputs um you're you're capable of getting the latest and greatest information um next question from Andy G how does the Precision of a single multimodal model for image and text embeddings compared to using a separate single modal model um and this is for Andre and for Howard do you all wna do you all want to answer yep uh do you want to start Howard or should I start uh please go
(43:45) ahead yeah so um I think the question implies uh that you're considering to use uh for single modality search right either a model that actually can handle multi mod modalities for single modality search or actually use single modality search for single modality uh retrieval applications because of course one of uh just to make sure we're all on the same page one of the big advantages of having a multimodel model is that you can actually retrieve uh you know search query with text and retrieve an image for example if you're talking about
(44:17) image and text at modalities and vice versa and you would not be able to that uh uh you would not be able to do that using single modality models right so there is already one advant but the other thing is that it's not necessarily that single model models will necessarily be better in single model uh single modality evaluations because for example uh when you look at specifically at image and text uh there's a lot of uh uh data uh uh online for example that has a text supervision text annotator together with the image
(44:52) and just by learning uh models that way that are already going to be multimodel you are actually able to leverage a lot of supervision which sometimes you're not able to do if you want to have uh manually rated data for uh images which is much more expensive so um and the other thing to mention is of course many of these things depend uh on the model that uh uh sorry on the data that the models were trained how large are the models generally we see the trend in the community of going to larger models more capable models that
(45:26) will do well across many different modalities uh rather than um having specialized modor fre modality of course you can also apply this technique called knowledge installation to take a really big model really capable one and to still on a smaller model to accomplish a task that is a little bit more restricted um or just to have a smaller model generally so um yeah these are some thoughts um Howard is there anything else you want wanted to add yeah um thank you Andre and I I one thing that um um that we observe as a
(46:06) trend is that um uh people are using the RMS or multimodel RMS um as a way to bridge in the gap between modalities so uh for instance take um the ev5 um model they they basically have the multimodel model uh LM to as an you know a way to take the input from any modality and then uh what they they were training is p purly training on text right but because the uh ml uh so the LM has already taken care of the input modalities so it effectively once uh you know by that I mean they map images and text and into the same embedding space
(46:53) so all you need to do now is to organize this embedding space based on text information so let's say if you are collaborate well you can you can you know really organize the um word of airplane versus fighter jets versus you know aircraft carrier in the space uh you know relatively then images of Air airplane fighter jets and aircraft carrier will be automatically organized because this a shared embedding space so that technique is actually very powerful um I think you know what we observe is that today uh you know I I so a couple
(47:35) of the um representative models so you have single modality models uh what we call do encoders where you just align them for image text um uh tasks uh alignment tasks like clip or align and they used to be state-of-the-art but now these uh single multimodel models started catching up um examples are POA and the e e5v um they you know I I think the trend is that we'll probably see uh the single multimodel models uh will achieve higher and high precision and close the gap and you know even surpassing the due encoder models um
(48:14) that uses separate single model components awesome fascinating I love those answers and thank you so much aie G for the great question um and with that I want to give a big virtual Round of Applause to all of our wonderful Q&A um experts who came here today who gave their time to to be able to to share more about what they've learned as well as to help answer our community questions we really appreciate all of you and your time um and thank you uh thank you so much again um the next section is going to be all about our
(48:47) code Labs um around embeddings and Vector databases um thank you so much to Mark McDonald for creating the notebooks and uh and Nott do you want to take it away to to explain them um folks on the call you can uh you can also uh kind of uh uh go off uh uh go off into the background while and not doing the the explanation of the code Labs thank you so much anant you might be on mute so so uh yep all right yeah so uh just to be efficient since we have uh we had an interesting conversation I'm going to make this section super short uh I'm
(49:38) sharing my screen now um this notebook is uh rag uh retrieval augmented generation here you will see how you can utilize the um the Gemini API and open source databases to build it a simple rag pipeline out so firstly we see um in the first section of the notebook we see how we make some synthetic data three three paragraphs and then we in the SEC part after that we see how we use chroma DB and open source in memory database to kind of uh index so um to Index this data and create um um create like so we you have a embedding uh we used
(50:20) embeddings um the to embed the text data and then persist it in the chroma DB and and after that we see uh we see that was the index sorry that was the generation embedding and indexing part then comes the the retrieval part of R where we see um how we can leverage this um um database uh to query take take a natural language text a query from the user uh and um the database internally embeds it and uh retrieves um retrieves the top K most uh similar results and then after that we see okay now that we have received uh s retrieved the um more
(51:01) similar results how can we use that to answer the question using the Gemini 2.0 flash um edlm and that's the last part of the collab where we do the generation part with the retrieved documents by the semantic search so that was um that's your very basic um R app now I'll move on to the second collab so moving on to the second collab um um yeah give me a second um yes so the second collab talks about um exploring the semantic aspect of embeddings to see how similarly or documents with similar meanings are grouped together in the Shad semantic
(51:44) space so in this collab we see how we Define some sentences um some synthetic sentences and we we embed all of them using the gecko embedding model by Google and we see how they look how the semantic similarities look in um through a heat map so in this heat map you see that on a scale of one to zero where the darker Shades represent values closer to one and the lighter ones represent closer to zero we see that documents which are more or less meaning the same uh are have a darker like shade which means they have much more semantically
(52:21) similar and of course the diagonal represents a document being compared to itself and document will always be more SE more semantic similar to itself of course then we see how others um other documents um are not very similar and you see lighter Shades there for instance um the quick brown fox jumps over the lazy dog this is very semantically similar to itself and a slightly misspelled version of itself uh however it's not very similar to the five boxing Wizards jumping quickly and so on because these mean two different
(52:58) things so that was um the collab on semantic similarity let me quickly go over and walk through the last collab which you're going to see so um we talked a lot about embeddings being used for semantic search proximate need to find similar documents Etc but there's another very important application of embeddings that is using embeddings for as as a feature extraction of sort uh for Downstream models and in this collab we see exactly that we kind of use an open source data set the news groups Text data set uh get the train and test
(53:35) uh splits for it uh pre-process the data set in the subsequent section to kind of make it nice where each of the text the the the news articles have various classes and we are basically taking the text and trying to classify it into the right um target class um so we see that um if you use the embedding models to create like a a embedding representation of the new text and then train a classifier on top to tune uh like us add a layer or two afterwards and only tune those two layers is very efficient since uh the whole the embeddings already
(54:15) carry very rich representations we use the classification task type for the embedding models and uh we add um uh uh we use that to uh as in kasas to basically um get the embedding representations uh for the text and then build a classification model with that and you see that we get a pretty good performance um um um in the um uh in in the task after that and you can also try it on our own handwritten texts so this is how you can use embeddings for um Downstream models and Beyond just semantic search so that'll be all off to
(54:54) your page U for the next section thank you so much anant and now we are going into the last and final section of today's uh of today's course we're going to be doing the pop quiz and so our first question which application are embeddings not the best suited for um is it retrieval augmented generation or rag is it simple rule-based systems is it anomaly detection or is it recommender systems what application are embeddings not the best suited for I'm going to countdown everybody grab a piece of paper and a pencil and jot down your
(55:34) answer or just type them out um someplace handy um five 4 3 2 1 and the answer is B simple rule based systems are not something that embeddings are best suited for um next question which of the following is a major advantage of scan over other approximate nearest neighbor algorithms is it a it's widely open- source and available um B that it's designed for high dimensional data and has excellent speed accuracy trade-offs is it C that scan only returns exact matches or is it D that it's based on a simple hashing
(56:11) technique and has low computational overhead which is a major advantage of scan over other approximate nearest neighbor search algorithms and I count down five 4 3 2 one and the answer is B it's designed for high-dimensional data and has excellent speed accuracy trade-offs so hopefully you are paying attention to the uh to when Allan was talking all about alloy Tob um and some of the some of the scan research um approaches there question three what are some of the major weaknesses of bag of words models for generating document
(56:50) embeddings is it a that they ignore word ordering and semantic meanings B that they're computationally expensive iive and require large amounts of data C that they cannot be used for semantic search or topic Discovery or D that they're only effective for short documents and fail to capture long range dependencies um so so what is uh what is some of the major weaknesses of bag of wordss models I'm going to count down five four 3 2 one and the answer is a that they ignore word ordering and semantic meanings question number four which of the
(57:29) following is a common challenge when using embeddings for search and how can it be addressed is it that embeddings cannot handle large data sets so you have to use a smaller data set that embeddings are always superior to traditional search so there's no need to kind of couple together search um is it that embeddings might not capture literal information very well so you have to combine with full text search or is it that embeddings change too frequently and you have to prevent updates from happening um so what is a common
(57:59) Challenge and how do you address it for embeddings I'm going to count down five 4 3 2 one hopefully no one thought that the best approach was to prevent new information from uh from happening or new things to occur the correct answer is C that embeddings might not capture literal information very well and so you should combine them with full text search question number five what is the primary advantage of using something like locality sensitive hashing for Vector search is it that a a guarantees finding the exact nearest neighbors B
(58:35) that it reduces the search Space by grouping similar items into hash buckets C the only method that works for high-dimensional vectors or D that it always provides the best trade-off between speed and accuracy what is the primary advantage of lsh for Vector search I'm going to count down five 4 3 2 one and the answer is B it reduces the search Space by grouping similar items into hash buckets so thank you so much to everyone hopefully everybody got 100% correct on the pop quiz um and we will see you tomorrow where we're going to be
(59:11) doing the next um instance of of learning for our kagle generative AI intensive course um really jazzed to see what questions you might have over the code Labs the white papers and the podcasts over the next 24 hours um and see you all tomorrow thank you so much