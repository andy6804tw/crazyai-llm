{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e88bcbf5-ab35-48fc-9db5-3cd3dd8a3332",
      "metadata": {
        "id": "e88bcbf5-ab35-48fc-9db5-3cd3dd8a3332"
      },
      "source": [
        "# Day 1 - 評估與結構化輸出\n",
        "\n",
        "## 概述\n",
        "\n",
        "歡迎回到 Kaggle 五天生成式 AI 課程！  \n",
        "在本筆記本中，你將學習一些評估語言模型輸出的方法。作為評估的一部分，你也會使用 Gemini 的結構化數據功能，將評估結果以 Python 類型的實例呈現出來。\n",
        "\n",
        "注意：這份筆記本的程式碼比 Day 1 的「提示工程」筆記本來得更重，因此它並非 Days 2 之後課程的先決條件；你可以選擇跳過或稍後再回來閱讀。如果你還未嘗試過「提示工程」筆記本，建議先從那邊開始，因為它介紹了與大型語言模型互動的基本原則。\n",
        "\n",
        "另外，也請參考那份有關 [評估大型語言模型](https://services.google.com/fh/files/blogs/neurips_evaluation.pdf) 的額外白皮書。\n",
        "\n",
        "## 求助說明\n",
        "\n",
        "若遇到常見問題，請參考 [FAQ 與疑難排解指南](https://www.kaggle.com/code/markishere/day-0-troubleshooting-and-faqs)。\n",
        "\n",
        "## 環境設置\n",
        "\n",
        "### 安裝 Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "fe0caff2-ed78-47c1-a5ec-a7bb27a10257",
      "metadata": {
        "id": "fe0caff2-ed78-47c1-a5ec-a7bb27a10257"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq \"google-genai==1.7.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "fdb930d8-e562-44c9-a14e-616ea427e48a",
      "metadata": {
        "id": "fdb930d8-e562-44c9-a14e-616ea427e48a"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2aa19e2-d7e1-464e-8513-a0b0ede1cd12",
      "metadata": {
        "id": "f2aa19e2-d7e1-464e-8513-a0b0ede1cd12"
      },
      "source": [
        "接著確認 SDK 版本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "220b5240-a37b-426c-b39d-28d629dddf72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "220b5240-a37b-426c-b39d-28d629dddf72",
        "outputId": "e738bb58-f96d-43be-cdeb-6187deb85266"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.7.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "genai.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d93434-b93b-4094-9afe-e6834c56aaa7",
      "metadata": {
        "id": "c0d93434-b93b-4094-9afe-e6834c56aaa7"
      },
      "source": [
        "### 設定 API 金鑰\n",
        "\n",
        "請先將你的 API 金鑰新增為 Kaggle Secret，名稱必須為 `GOOGLE_API_KEY`。  \n",
        "若你還沒有 API 金鑰，可前往 [AI Studio](https://aistudio.google.com/app/apikey) 取得，詳情請參考 [Gemini API 文件](https://ai.google.dev/gemini-api/docs/api-key)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ab8c78f5-6a26-4b84-854a-cb5ada1d5891",
      "metadata": {
        "id": "ab8c78f5-6a26-4b84-854a-cb5ada1d5891"
      },
      "outputs": [],
      "source": [
        "# from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# client = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))\n",
        "GOOGLE_API_KEY = 'AIzaSyDTNvIsXVmsfJFEdFZNc4W2IE0dgsNN9Jk'\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37c1af9-6a37-424d-bb35-670259ea8be7",
      "metadata": {
        "id": "d37c1af9-6a37-424d-bb35-670259ea8be7"
      },
      "source": [
        "若出現類似 `No user secrets exist for kernel id ...` 的錯誤，請依照說明透過 `Add-ons` → `Secrets` 新增並啟用你的金鑰。\n",
        "\n",
        "![啟用 GOOGLE_API_KEY 的截圖](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)\n",
        "\n",
        "### 自動重試機制\n",
        "\n",
        "由於本 codelab 會發送大量請求，因此請設置自動重試機制，確保當每分鐘配額達到上限時能自動重試。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "b19912db-f19f-4dee-b901-b8e4f8d8ca1f",
      "metadata": {
        "id": "b19912db-f19f-4dee-b901-b8e4f8d8ca1f"
      },
      "outputs": [],
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
        "  genai.models.Models.generate_content = retry.Retry(\n",
        "      predicate=is_retriable)(genai.models.Models.generate_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bc8fef-07f5-4cde-8aae-4cc5e6559e60",
      "metadata": {
        "id": "f0bc8fef-07f5-4cde-8aae-4cc5e6559e60"
      },
      "source": [
        "## 評估\n",
        "\n",
        "在實際應用大型語言模型時，了解其表現如何非常重要。由於模型具備開放式生成能力，許多任務的評估都比較困難。在本筆記本中，你將逐步學習如何評估模型輸出，以及瞭解它們的表現。\n",
        "\n",
        "以下範例將使用 [Gemini 1.5 Pro 技術報告](https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf) 作為文件，首先下載 PDF 至筆記本環境，並上傳供 Gemini API 使用。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "cf8ec8bc-ba02-4044-b920-d0fa678067b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf8ec8bc-ba02-4044-b920-d0fa678067b1",
        "outputId": "326ed9cf-220c-44ae-a2b7-20833413f096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-04 05:27:32 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n"
          ]
        }
      ],
      "source": [
        "!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n",
        "\n",
        "document_file = client.files.upload(file='gemini.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c06542e-faa4-4164-9287-4db6ccfb6348",
      "metadata": {
        "id": "8c06542e-faa4-4164-9287-4db6ccfb6348"
      },
      "source": [
        "### 文件摘要\n",
        "\n",
        "這裡的摘要請求相當基本，主要針對訓練內容，但未提供其他具體指示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "27c44f8b-c3ff-4b8d-8614-37fd237c2ef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "27c44f8b-c3ff-4b8d-8614-37fd237c2ef0",
        "outputId": "3fb36a65-cf1b-49c4-d687-17349b7ebdaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "好的，讓我來總結一下 Gemini 1.5 Pro 的訓練過程，並用繁體中文回答：\n\n根據您提供的文件，Gemini 1.5 Pro 的訓練過程可以概括為以下幾個關鍵點：\n\n1.  **架構：** Gemini 1.5 Pro 是一個基於 Transformer 的稀疏混合專家模型 (Sparse Mixture-of-Experts, MoE)。MoE 模型使用一個學習到的路由函數，將輸入導向模型參數的一個子集進行處理。這使得模型能夠擴大總參數數量，同時保持每個輸入激活的參數數量恆定。\n\n2.  **訓練資料：** 模型在多個 Google TPUv4 加速器的 4096 晶片上進行訓練，這些加速器分佈在多個資料中心。訓練資料集包含來自多個不同領域的資料，包括網頁文件、程式碼，並整合了圖像、音訊和影片內容。\n\n3.  **預訓練：** 模型首先在一個大型的多模態和多語言資料集上進行預訓練。\n\n4.  **指令微調 (Instruction Tuning)：** 在預訓練之後，模型會進行指令微調，使用包含配對的指令和適當回應的多模態資料集。此外，還會根據人類偏好資料進行進一步的調整。\n\n5.  **長文本處理能力：** Gemini 1.5 Pro 包含一系列重要的架構變更，使其能夠處理長達 1000 萬個 token 的輸入，而不會降低效能。\n\n6.  **安全措施：** 模型的安全風險透過監督式微調 (SFT) 和基於人類回饋的強化學習 (RLHF) 進行緩解。\n\n總體來說，Gemini 1.5 Pro 的訓練過程結合了先進的模型架構、大規模的資料集、以及精細的微調策略，使其能夠在多種模態上實現高效能，並具備處理極長文本序列的能力。"
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "request = '請告訴我這裡所使用的訓練過程。並使用繁體中文回答。'\n",
        "\n",
        "def summarise_doc(request: str) -> str:\n",
        "  \"\"\"對上傳的文件執行摘要請求。\"\"\"\n",
        "  # 設定低溫以穩定輸出\n",
        "  config = types.GenerateContentConfig(temperature=0.0)\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=config,\n",
        "      contents=[request, document_file],\n",
        "  )\n",
        "  return response.text\n",
        "\n",
        "summary = summarise_doc(request)\n",
        "Markdown(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定義評估器\n",
        "\n",
        "針對此任務，你可以評估多個面向，如模型是否遵循指令（Instruction following）、是否只依據上下文回答（Groundedness）、摘要是否精簡（Conciseness）與流暢易讀（Fluency）。\n",
        "\n",
        "你可以像指導人類評分者一樣，為模型設定明確的定義與評分標準。下例中，定義了一個評估代理，使用預設的「摘要」提示來評分生成的摘要。\n",
        "\n",
        "下列提示會依據以下標準：\n",
        "- 指令遵循：是否完全理解並滿足任務要求。\n",
        "- 依據性：僅根據上下文提供資訊。\n",
        "- 簡潔性：摘要是否能在不遺漏關鍵資訊下，精簡呈現原文內容。\n",
        "- 流暢度：回答是否組織良好且易於閱讀。\n",
        "\n",
        "評分標準如下：\n",
        "- 5：非常好 – 完全遵循指令，依據性強，摘要精簡且流暢。\n",
        "- 4：好 – 遵循指令、依據性強、摘要完整，但流暢度略欠。\n",
        "- 3：普通 – 大部分遵循指令，但略有遺漏或不夠流暢。\n",
        "- 2：差 – 回答依據性尚可，但明顯未滿足指令要求。\n",
        "- 1：非常差 – 回答不符合指令，資訊錯誤且不依據上下文。\n",
        "\n",
        "> 若想查看更多涵蓋「依據性（groundedness）」、「安全性（safety）」、「連貫性（coherence）」等面向的預先撰寫評估提示語，可以參考 Google Cloud 文件中這份[完整的模型評估提示語清單](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates)。"
      ],
      "metadata": {
        "id": "2nSRkSm1R_xx"
      },
      "id": "2nSRkSm1R_xx"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "3f6c2003-942a-46ec-b746-d140b9c6dd77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "3f6c2003-942a-46ec-b746-d140b9c6dd77",
        "outputId": "17ac01ec-cc6e-4e67-efa9-217e915967fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "STEP 1: 評估指令遵循、依據性、簡潔性與流暢度。\n* 指令遵循：AI 回應正確理解使用者要求，以繁體中文回答並總結 Gemini 1.5 Pro 的訓練過程。\n* 依據性：AI 回應僅包含使用者上傳文件內的資訊，未引用外部資料。\n* 簡潔性：AI 回應在不遺漏關鍵資訊的前提下，已盡可能精簡摘要內容。\n* 流暢度：AI 回應結構清晰，易於閱讀。\n\nSTEP 2: 根據上述標準給予總分。\n綜合以上評估，我認為此回應的摘要品質非常好，完全遵循指令、依據性強、精簡且流暢。\n\n總分：5\n"
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import enum\n",
        "\n",
        "# 定義評分等級\n",
        "class SummaryRating(enum.Enum):\n",
        "  VERY_GOOD = '5'\n",
        "  GOOD = '4'\n",
        "  OK = '3'\n",
        "  BAD = '2'\n",
        "  VERY_BAD = '1'\n",
        "\n",
        "# 定義評估提示\n",
        "SUMMARY_PROMPT = \"\"\"\\\n",
        "# 指令\n",
        "你是一位專業評估者，你的任務是評估 AI 模型生成的回應品質。\n",
        "我們會提供使用者輸入與 AI 生成的回應。\n",
        "請先仔細閱讀使用者輸入，然後根據下方的評分標準與評估步驟，評估回應品質，並一步一步說明你的評分過程，最後僅從評分標準中選擇分數。\n",
        "\n",
        "# 評估標準\n",
        "## 指標定義\n",
        "本次評估針對摘要品質，評估重點在於模型是否能正確、精簡地摘要原文。回應內容應比原文更精簡，且不包含上下文中未出現的資訊。\n",
        "\n",
        "## 評分準則\n",
        "指令遵循：回答必須清楚理解並滿足摘要任務要求。\n",
        "依據性：回應僅應包含原文內的資訊，不應引用外部資料。\n",
        "簡潔性：回應應在不遺漏關鍵資訊下，盡可能精簡摘要內容。\n",
        "流暢度：回應結構清晰，易於閱讀。\n",
        "\n",
        "## 評分規範\n",
        "5: 非常好 – 摘要完全遵循指令、依據性強、精簡且流暢。\n",
        "4: 好 – 摘要基本符合要求，但流暢度略欠。\n",
        "3: 普通 – 摘要大部分符合，但缺乏完整性或流暢度不足。\n",
        "2: 差 – 摘要雖有依據，但未完全遵循指令。\n",
        "1: 非常差 – 摘要完全未依據上下文，內容錯誤。\n",
        "\n",
        "# 評估步驟\n",
        "STEP 1: 分別評估指令遵循、依據性、簡潔性與流暢度。\n",
        "STEP 2: 根據上述標準給予總分。\n",
        "\n",
        "# 使用者輸入與 AI 回應\n",
        "## 使用者輸入\n",
        "\n",
        "### 提示\n",
        "{prompt}\n",
        "\n",
        "## AI 生成回應\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "def eval_summary(prompt: str, ai_response: str):\n",
        "  \"\"\"根據使用者提示與 AI 回應進行評估。\"\"\"\n",
        "  chat = client.chats.create(model='gemini-2.0-flash')\n",
        "  response = chat.send_message(\n",
        "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
        "  )\n",
        "  verbose_eval = response.text\n",
        "\n",
        "  structured_output_config = types.GenerateContentConfig(\n",
        "      response_mime_type=\"text/x.enum\",\n",
        "      response_schema=SummaryRating,\n",
        "  )\n",
        "  response = chat.send_message(\n",
        "      message=\"請轉換為最終分數。\",\n",
        "      config=structured_output_config,\n",
        "  )\n",
        "  structured_eval = response.parsed\n",
        "\n",
        "  return verbose_eval, structured_eval\n",
        "\n",
        "text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\n",
        "Markdown(text_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在這個例子中，模型產生了一段文字型的解釋，用在一個對話式的情境裡。這段完整的文字回應對於「人類理解」來說很有幫助，同時也提供模型一個「做筆記」的空間，讓它在評估文本並產出最終分數時，可以一邊思考、一邊整理資訊。\n",
        "\n",
        "這種「做筆記」或「思考中」的策略，通常很適合用在 **自回歸模型（auto-regressive models）** 上，因為這類模型會在每一步生成文字的同時，把之前的輸出再餵回模型，作為下一步的輸入。也就是說，這些「工作筆記」會被拿來幫助產出最後的結果。\n",
        "\n",
        "到了下一輪對話時，模型會把這段文字回應轉成 **結構化的輸出**。如果你想要 **彙總分數** 或用程式處理結果，那就不建議從文字裡硬解析資訊。這裡使用了一個叫做 `SummaryRating` 的 schema（資料格式），模型就會把對話紀錄轉成這個 `SummaryRating` 枚舉型別的實例。\n"
      ],
      "metadata": {
        "id": "-SsZJ9xXTMp3"
      },
      "id": "-SsZJ9xXTMp3"
    },
    {
      "cell_type": "code",
      "source": [
        "struct_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR-MIXHVSNpf",
        "outputId": "e47f8d48-d4a2-4cd3-d3e5-30766dae3025"
      },
      "id": "TR-MIXHVSNpf",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SummaryRating.VERY_GOOD: '5'>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 改進摘要提示\n",
        "\n",
        "Gemini 模型在摘要任務上通常表現不錯，但你可以試著調整提示內容來影響結果。請考慮以下改進方向：\n",
        "* 明確要求摘要字數，例如「請用 50 字左右簡述」。\n",
        "* 要求回應包含特定資訊。\n",
        "* 詢問不在文件中的資訊。\n",
        "* 調整摘要風格，如「像對 5 歲小孩解釋 (Explain like I'm 5)」或「以完整技術深度說明 (with full technical depth)」。\n",
        "\n",
        "試著以下面的提示運行看看："
      ],
      "metadata": {
        "id": "wtF-EjclgoMl"
      },
      "id": "wtF-EjclgoMl"
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = \"Explain like I'm 5 模型訓練過程。\"\n",
        "# 你可以試試：\n",
        "# 1. 請用簡短的文字說明訓練過程。\n",
        "# 2. 請以簡單易懂的方式解釋訓練過程。\n",
        "# 3. 請以專業角度解釋模型的訓練方法。\n",
        "\n",
        "if not new_prompt:\n",
        "  raise ValueError(\"請設定一個新的摘要提示。\")\n",
        "\n",
        "def run_and_eval_summary(prompt: str):\n",
        "  \"\"\"使用新的提示生成並評估摘要。\"\"\"\n",
        "  summary = summarise_doc(prompt)\n",
        "  display(Markdown(summary + '\\n-----'))\n",
        "\n",
        "  text, struct = eval_summary(prompt, summary)\n",
        "  display(Markdown(text + '\\n-----'))\n",
        "  print(struct)\n",
        "\n",
        "run_and_eval_summary(new_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "LrXHjAUQT6D4",
        "outputId": "583df0ca-94d3-4177-c3a0-bd0f02b0a260"
      },
      "id": "LrXHjAUQT6D4",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "好的，讓我用簡單的方式解釋一下模型訓練的過程，就像跟五歲小孩解釋一樣：\n\n想像一下，我們想教電腦一個新的遊戲，這個遊戲叫做「猜猜我是誰」。\n\n1.  **準備教材：**\n    *   首先，我們要準備很多很多的例子，告訴電腦「猜猜我是誰」這個遊戲是怎麼玩的。\n    *   這些例子就像是給電腦看的書，書裡面有很多圖片和文字，告訴電腦什麼樣的圖片和文字代表什麼意思。\n    *   例如，書裡面會有很多動物的圖片，然後告訴電腦：「這是小狗」、「這是小貓」、「這是小鳥」等等。\n\n2.  **開始學習：**\n    *   然後，我們把這些書給電腦看，電腦會開始努力學習。\n    *   電腦會一邊看圖片和文字，一邊猜猜看這是什麼。\n    *   如果電腦猜對了，我們就給它一個獎勵，告訴它「你真棒！猜對了！」。\n    *   如果電腦猜錯了，我們就告訴它「猜錯了，再試一次！」，然後電腦會再努力想一想，下次要怎麼猜才對。\n\n3.  **不斷練習：**\n    *   電腦會一直不斷地看書、猜猜看、得到獎勵或被糾正，這樣它就會越來越厲害。\n    *   就像我們學東西一樣，練習越多，就學得越好。\n\n4.  **考試驗收：**\n    *   等到電腦學得差不多了，我們就給它一個考試，看看它是不是真的學會了。\n    *   考試的時候，我們會給電腦看一些它沒看過的圖片和文字，讓它猜猜看。\n    *   如果電腦在考試的時候也能猜對很多，那就表示它真的學會了「猜猜我是誰」這個遊戲了！\n\n所以，模型訓練就像是教電腦玩遊戲，我們要準備很多教材，讓電腦不斷學習和練習，最後通過考試，才能證明它真的學會了。\n-----"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "STEP 1:\n*   指令遵循：AI 回應有理解使用者提問，並以適合五歲兒童理解的方式解釋模型訓練過程。\n*   依據性：AI 回應未引用外部資料，所有內容均為基於對模型訓練的理解所產生的解釋。\n*   簡潔性：AI 回應為了便於理解，使用了較多比喻和例子，因此在簡潔性方面略有不足，但考慮到目標受眾為五歲兒童，此種表達方式可以接受。\n*   流暢度：AI 回應結構清晰，使用了分點敘述，易於閱讀和理解。\n\nSTEP 2:\n綜合以上評估，我認為 AI 回應在指令遵循、依據性和流暢度方面表現良好，但在簡潔性方面略有不足。考慮到目標受眾，我給予總分 4 分。\n\n總分：4\n\n-----"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SummaryRating.GOOD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評估實作\n",
        "\n",
        "評估有許多實際應用，例如：\n",
        "* 快速對一組測試文件迭代提示設計。\n",
        "* 比較不同模型之間的效能，找到成本與效能的最佳平衡。\n",
        "* 在生產系統中推送模型或提示修改時，確保品質不退步。\n",
        "\n",
        "以下部分示範兩種評估方式。\n",
        "\n",
        "### 單點評估（Pointwise evaluation）\n",
        "\n",
        "單點評估指針對單一輸入／輸出對進行評分，例如「這回答好還是不錯？」。\n",
        "\n",
        "在此練習中，你將針對一組問題嘗試不同的指導提示(guidance prompts)。"
      ],
      "metadata": {
        "id": "DKSvWNoWjxSN"
      },
      "id": "DKSvWNoWjxSN"
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "# 試試以下指導說明，或自行編輯與新增：\n",
        "terse_guidance = \"請用一句話（或接近一句話）回答下列問題。\"\n",
        "moderate_guidance = \"請簡短回答下列問題，如有需要可附上引用，但只要足夠回答問題即可。\"\n",
        "cited_guidance = \"請詳細回答下列問題，並引用文件內容與提供額外背景資訊。\"\n",
        "guidance_options = {\n",
        "    'Terse': terse_guidance,\n",
        "    'Moderate': moderate_guidance,\n",
        "    'Cited': cited_guidance,\n",
        "}\n",
        "\n",
        "questions = [\n",
        "    # 你可以取消註解以下問題或新增自己的問題。\n",
        "    # 更多問題會花較長時間，但可獲得更高信心的結果。\n",
        "    # \"用什麼指標來評估長上下文效能？\",\n",
        "    \"模型在程式碼任務上的表現如何？\",\n",
        "    \"模型有多少層？\",\n",
        "    # \"為何叫做 Gemini？\",\n",
        "]\n",
        "\n",
        "if not questions:\n",
        "  raise NotImplementedError('請新增一些問題以進行評估！')\n",
        "\n",
        "@functools.cache\n",
        "def answer_question(question: str, guidance: str = '') -> str:\n",
        "  \"\"\"使用上傳的文件與指導提示，生成問題的答案。\"\"\"\n",
        "  config = types.GenerateContentConfig(\n",
        "      temperature=0.0,\n",
        "      system_instruction=guidance,\n",
        "  )\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-2.0-flash',\n",
        "      config=config,\n",
        "      contents=[question, document_file],\n",
        "  )\n",
        "  return response.text\n",
        "\n",
        "answer = answer_question(questions[0], terse_guidance)\n",
        "Markdown(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "-_FlD35qhzQD",
        "outputId": "26fee42a-496d-4a44-c1cc-33680b49aa65"
      },
      "id": "-_FlD35qhzQD",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Gemini 1.5 Pro shows an improvement in coding capabilities compared to previous Gemini models, surpassing Gemini 1.0 Ultra on the Natural2Code benchmark.\n"
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在，請設置一個問答評估器，類似之前的方式，但使用 [單點問答評估提示](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_question_answering_quality)。\n"
      ],
      "metadata": {
        "id": "78TARNsCzwNb"
      },
      "id": "78TARNsCzwNb"
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "QA_PROMPT = \"\"\"\\\n",
        "# 指令\n",
        "你是一位專業評估者，你的任務是評估 AI 模型生成回應的品質。\n",
        "我們會提供使用者提示與 AI 生成的回應。\n",
        "請先仔細閱讀使用者提示，再根據下列評估標準對回應品質進行評分。請依照評分規範與評估步驟，逐步說明你的評分過程，並僅選擇 5、4、3、2 或 1 其中一個分數。\n",
        "\n",
        "## 評估標準\n",
        "- 指令遵循：回應展現出對問答任務指令的清楚理解，滿足所有要求。\n",
        "- 依據性：回應僅包含使用者提示中的資訊，不引用外部內容。\n",
        "- 完整性：回應能充分回答問題，具備足夠細節。\n",
        "- 流暢度：回應組織良好、易於閱讀。\n",
        "\n",
        "## 評分規範\n",
        "5：非常好 – 回應完全遵循指令、依據性強、完整且流暢。\n",
        "4：好 – 回應遵循指令、依據性強、完整，但流暢度略遜。\n",
        "3：普通 – 回應大致符合要求，但部分缺失或不夠流暢。\n",
        "2：差 – 回應未能充分遵循指令，或內容不完整。\n",
        "1：非常差 – 回應完全不符合指令，內容錯誤且無依據。\n",
        "\n",
        "## 評估步驟\n",
        "STEP 1：根據指令遵循、依據性、完整性及流暢度評估回應。\n",
        "STEP 2：依據上述標準給出最終分數。\n",
        "\n",
        "# 使用者提示與 AI 生成回應\n",
        "## 使用者提示\n",
        "### 提示\n",
        "{prompt}\n",
        "\n",
        "## AI 生成回應\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "class AnswerRating(enum.Enum):\n",
        "  VERY_GOOD = '5'\n",
        "  GOOD = '4'\n",
        "  OK = '3'\n",
        "  BAD = '2'\n",
        "  VERY_BAD = '1'\n",
        "\n",
        "@functools.cache\n",
        "def eval_answer(prompt, ai_response, n=1):\n",
        "  \"\"\"根據使用者提示與 AI 回應評估問答品質。\"\"\"\n",
        "  chat = client.chats.create(model='gemini-2.0-flash')\n",
        "  response = chat.send_message(\n",
        "      message=QA_PROMPT.format(prompt=[prompt, document_file], response=ai_response)\n",
        "  )\n",
        "  verbose_eval = response.text\n",
        "\n",
        "  structured_output_config = types.GenerateContentConfig(\n",
        "      response_mime_type=\"text/x.enum\",\n",
        "      response_schema=AnswerRating,\n",
        "  )\n",
        "  response = chat.send_message(\n",
        "      message=\"請轉換為最終分數。\",\n",
        "      config=structured_output_config,\n",
        "  )\n",
        "  structured_eval = response.parsed\n",
        "\n",
        "  return verbose_eval, structured_eval\n",
        "\n",
        "text_eval, struct_eval = eval_answer(prompt=questions[0], ai_response=answer)\n",
        "display(Markdown(text_eval))\n",
        "print(struct_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "slbaivmMkDen",
        "outputId": "79f6eedf-cc28-4de4-ce1f-a739f3b34809"
      },
      "id": "slbaivmMkDen",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "STEP 1：\n* 指令遵循：回應完全遵循指令，根據提供的文件內容回答了模型在程式碼任務上的表現。\n* 依據性：回應僅包含使用者提示中的檔案內容，沒有引用任何外部資訊。\n* 完整性：回應充分回答了問題，提供了模型在程式碼任務上的整體表現、具體指標、長文脈能力、程式碼資料集上的困惑度與檢索，以及程式碼生成等方面的資訊。\n* 流暢度：回應的組織良好、易於閱讀，各項指標都清晰呈現。\n\nSTEP 2：\n綜合考量以上因素，我給予此回應 5 分。\n\n最終分數：5\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnswerRating.VERY_GOOD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接著，將評估任務在迴圈中執行。注意，此處評估代理不會接收到指導提示，目標是根據使用者問題找出最佳回應，而非依照開發者指令。\n"
      ],
      "metadata": {
        "id": "O-eeht4u0Kcl"
      },
      "id": "O-eeht4u0Kcl"
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import itertools\n",
        "\n",
        "# 為了減少誤差，請設定每個任務重複執行的次數 (NUM_ITERATIONS)，次數越多結果越穩定，但耗時較長，建議先嘗試 1 或 2 次。\n",
        "NUM_ITERATIONS = 1\n",
        "\n",
        "scores = collections.defaultdict(int)\n",
        "responses = collections.defaultdict(list)\n",
        "\n",
        "for question in questions:\n",
        "  display(Markdown(f'## {question}'))\n",
        "  for guidance, guide_prompt in guidance_options.items():\n",
        "    for n in range(NUM_ITERATIONS):\n",
        "      # 生成回應\n",
        "      answer = answer_question(question, guide_prompt)\n",
        "      # 評估回應 (注意，此處不傳指導提示 guidance_options 代表模型不會知道這段回答是精簡、適中、詳細)\n",
        "      written_eval, struct_eval = eval_answer(question, answer, n)\n",
        "      # 目的是讓模型在QA_PROMPT評估下評估這個問題適合用哪種回答。\n",
        "      print(f'{guidance}: {struct_eval}')\n",
        "      # 儲存數值分數\n",
        "      scores[guidance] += int(struct_eval.value)\n",
        "      # 儲存回應內容以便檢視\n",
        "      responses[(guidance, question)].append((answer, written_eval))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "KlCXO_9Vz1Ux",
        "outputId": "1727980f-8b26-46e8-c523-be5f9845ad3f"
      },
      "id": "KlCXO_9Vz1Ux",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 模型在程式碼任務上的表現如何？"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terse: AnswerRating.GOOD\n",
            "Moderate: AnswerRating.VERY_GOOD\n",
            "Cited: AnswerRating.VERY_GOOD\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 模型有多少層？"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terse: AnswerRating.BAD\n",
            "Moderate: AnswerRating.VERY_GOOD\n",
            "Cited: AnswerRating.GOOD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後，計算每個指導提示的平均分數："
      ],
      "metadata": {
        "id": "OvnT4FibC2D0"
      },
      "id": "OvnT4FibC2D0"
    },
    {
      "cell_type": "code",
      "source": [
        "for guidance, score in scores.items():\n",
        "  avg_score = score / (NUM_ITERATIONS * len(questions))\n",
        "  nearest = AnswerRating(str(round(avg_score)))\n",
        "  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElcZblsx0CHC",
        "outputId": "c0f9e4f7-23df-4acc-96c2-77c3eedea919"
      },
      "id": "ElcZblsx0CHC",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terse: 2.50 - BAD\n",
            "Moderate: 4.00 - GOOD\n",
            "Cited: 4.50 - GOOD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 配對評估 (Pairwise evaluation)\n",
        "\n",
        "單點評估的分級可能過於粗略，若你希望更精細地比較兩個回應，可使用配對評估。這是排序演算法中的重要步驟，可用來對提示進行排序。\n",
        "\n",
        "以下範例使用 [配對問答評估提示](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pairwise_question_answering_quality) 來比較兩個回應。\n"
      ],
      "metadata": {
        "id": "qjyjE56vC89F"
      },
      "id": "qjyjE56vC89F"
    },
    {
      "cell_type": "code",
      "source": [
        "QA_PAIRWISE_PROMPT = \"\"\"\\\n",
        "# 指令\n",
        "你是一位專業評估者，你的任務是評估兩個 AI 模型生成回應的品質。我們會提供使用者提示與一對 AI 生成回應（回應 A 與回應 B）。\n",
        "請先仔細閱讀使用者提示，再根據下列評估標準對回應進行評分。\n",
        "\n",
        "你將先分別評估回應 A 與回應 B，然後逐步說明你的判斷過程，並根據評分規範比較兩者，最終輸出 \"A\"、\"SAME\" 或 \"B\" 來表示哪個回應較佳。\n",
        "\n",
        "# 評估\n",
        "## 指標定義\n",
        "你將評估問答品質，重點在於回應是否完整回答使用者問題，且僅根據上下文提供資訊。\n",
        "\n",
        "## 評估標準\n",
        "- 指令遵循：回應是否展現出對問答任務的清楚理解並滿足所有要求。\n",
        "- 依據性：回應是否僅包含使用者提示中的資訊，不引用外部資料。\n",
        "- 完整性：回應是否完整回答問題，具備足夠細節。\n",
        "- 流暢度：回應是否組織良好且易讀。\n",
        "\n",
        "## 評分規範\n",
        "\"A\": 回應 A 比回應 B 更符合標準。\n",
        "\"SAME\": 兩個回應表現大致相同。\n",
        "\"B\": 回應 B 比回應 A 更符合標準。\n",
        "\n",
        "# 評估步驟\n",
        "STEP 1：分別根據指令遵循、依據性、完整性與流暢度評估回應 A 與回應 B。\n",
        "STEP 2：比較兩者整體表現。\n",
        "STEP 3：輸出你對該對比的偏好，請選擇 \"A\"、\"SAME\" 或 \"B\"，並說明理由。\n",
        "\n",
        "# 使用者提示與 AI 回應\n",
        "## 使用者提示\n",
        "### 提示\n",
        "{prompt}\n",
        "\n",
        "# AI 生成回應\n",
        "### 回應 A\n",
        "{baseline_model_response}\n",
        "\n",
        "### 回應 B\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "class AnswerComparison(enum.Enum):\n",
        "  A = 'A'\n",
        "  SAME = 'SAME'\n",
        "  B = 'B'\n",
        "\n",
        "@functools.cache\n",
        "def eval_pairwise(prompt, response_a, response_b, n=1):\n",
        "  \"\"\"比較同一提示下兩個回應的優劣。\"\"\"\n",
        "  chat = client.chats.create(model='gemini-2.0-flash')\n",
        "  response = chat.send_message(\n",
        "      message=QA_PAIRWISE_PROMPT.format(\n",
        "          prompt=[prompt, document_file],\n",
        "          baseline_model_response=response_a,\n",
        "          response=response_b)\n",
        "  )\n",
        "  verbose_eval = response.text\n",
        "\n",
        "  structured_output_config = types.GenerateContentConfig(\n",
        "      response_mime_type=\"text/x.enum\",\n",
        "      response_schema=AnswerComparison,\n",
        "  )\n",
        "  response = chat.send_message(\n",
        "      message=\"請轉換為最終選擇。\",\n",
        "      config=structured_output_config,\n",
        "  )\n",
        "  structured_eval = response.parsed\n",
        "\n",
        "  return verbose_eval, structured_eval\n",
        "\n",
        "question = questions[0]\n",
        "answer_a = answer_question(question, terse_guidance)\n",
        "answer_b = answer_question(question, cited_guidance)\n",
        "\n",
        "text_eval, struct_eval = eval_pairwise(\n",
        "    prompt=question,\n",
        "    response_a=answer_a,\n",
        "    response_b=answer_b,\n",
        ")\n",
        "\n",
        "display(Markdown(text_eval))\n",
        "print(struct_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "7SLWc81yC4Vb",
        "outputId": "e36a779e-467a-4cb8-be51-94ab291de5e1"
      },
      "id": "7SLWc81yC4Vb",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "STEP 1：\n回應 A：回應 A 簡潔地總結了模型在程式碼任務上的表現，但資訊量較少，未能充分利用提供的文件。\n回應 B：回應 B 更詳盡地總結了模型在程式碼任務上的表現，提供了具體的指標和能力描述，更全面地回答了問題。\n\nSTEP 2：\n回應 B 在指令遵循和完整性方面優於回應 A，提供了更豐富、更深入的資訊，且組織良好，易於閱讀。\n\nSTEP 3：\nB\n回應 B 更詳盡地回答了使用者提示，提供了具體的指標和能力描述，更全面地回答了問題，因此比回應 A 更符合標準。"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnswerComparison.B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "有了配對評估器後，你只需進行提示間的比較即可對提示進行排序。以下程式碼示範如何對多個提示根據配對評估結果進行排序：\n"
      ],
      "metadata": {
        "id": "aS7T0GOOdAVF"
      },
      "id": "aS7T0GOOdAVF"
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.total_ordering\n",
        "class QAGuidancePrompt:\n",
        "  \"\"\"封裝問答提示或系統指令的類別。\"\"\"\n",
        "\n",
        "  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n",
        "    \"\"\"建立提示。請提供用於評估的問題與評估次數。\"\"\"\n",
        "    self.prompt = prompt\n",
        "    self.questions = questions\n",
        "    self.n = n_comparisons\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.prompt\n",
        "\n",
        "  def _compare_all(self, other):\n",
        "    \"\"\"針對所有問題比較兩個提示，回傳平均分（四捨五入）。\"\"\"\n",
        "    results = [self._compare_n(other, q) for q in questions]\n",
        "    mean = sum(results) / len(results)\n",
        "    return round(mean)\n",
        "\n",
        "  def _compare_n(self, other, question):\n",
        "    \"\"\"針對某個問題進行 n 次比較，回傳平均分。\"\"\"\n",
        "    results = [self._compare(other, question, n) for n in range(self.n)]\n",
        "    mean = sum(results) / len(results)\n",
        "    return mean\n",
        "\n",
        "  def _compare(self, other, question, n=1):\n",
        "    \"\"\"針對單一問題比較兩個提示。\"\"\"\n",
        "    answer_a = answer_question(question, self.prompt)\n",
        "    answer_b = answer_question(question, other.prompt)\n",
        "    _, result = eval_pairwise(\n",
        "        prompt=question,\n",
        "        response_a=answer_a,\n",
        "        response_b=answer_b,\n",
        "        n=n,\n",
        "    )\n",
        "    if result is AnswerComparison.A:\n",
        "      return 1\n",
        "    elif result is AnswerComparison.B:\n",
        "      return -1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if not isinstance(other, QAGuidancePrompt):\n",
        "      return NotImplemented\n",
        "    return self._compare_all(other) == 0\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    if not isinstance(other, QAGuidancePrompt):\n",
        "      return NotImplemented\n",
        "    return self._compare_all(other) < 0\n",
        "\n",
        "terse_prompt = QAGuidancePrompt(terse_guidance, questions)\n",
        "moderate_prompt = QAGuidancePrompt(moderate_guidance, questions)\n",
        "cited_prompt = QAGuidancePrompt(cited_guidance, questions)\n",
        "\n",
        "sorted_results = sorted([terse_prompt, moderate_prompt, cited_prompt], reverse=True)\n",
        "for i, p in enumerate(sorted_results):\n",
        "  if i:\n",
        "    print('---')\n",
        "  print(f'#{i+1}: {p}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn9IT1OUdAtw",
        "outputId": "34a211e8-e6c4-4784-f423-55ad085f16a5"
      },
      "id": "rn9IT1OUdAtw",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1: 請簡短回答下列問題，如有需要可附上引用，但只要足夠回答問題即可。\n",
            "---\n",
            "#2: 請詳細回答下列問題，並引用文件內容與提供額外背景資訊。\n",
            "---\n",
            "#3: 請用一句話（或接近一句話）回答下列問題。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC.\n",
        "\n",
        "# @title 根據 Apache License, Version 2.0（以下簡稱「授權條款」）取得授權；\n",
        "# 除非遵循授權條款，否則不得使用此檔案。\n",
        "# 你可以從以下網址取得授權條款副本：\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# 除非適用法律要求或書面同意，本軟體皆按「現狀」提供，\n",
        "# 不附帶任何形式的明示或暗示保證。\n",
        "# 請參閱授權條款以了解具體權限與限制。"
      ],
      "metadata": {
        "id": "F45otLIcdE7-"
      },
      "id": "F45otLIcdE7-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}