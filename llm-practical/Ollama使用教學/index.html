
<!doctype html>
<html lang="zh-TW" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Tsai Yi Lin">
      
      
        <link rel="canonical" href="https://andy6804tw.github.io/crazyai-llm/llm-practical/Ollama%E4%BD%BF%E7%94%A8%E6%95%99%E5%AD%B8/">
      
      
        <link rel="prev" href="../../google-genai-course/2025-04-04-day5/">
      
      
        <link rel="next" href="../Ollama%20API%E6%95%B4%E5%90%88OpenAI/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.42">
    
    
      
        <title>Ollama使用教學 - 全民瘋AI系列 [大語言模型應用與實戰]</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ollama" class="md-skip">
          跳轉到
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="頁首">
    <a href="../.." title="全民瘋AI系列 [大語言模型應用與實戰]" class="md-header__button md-logo" aria-label="全民瘋AI系列 [大語言模型應用與實戰]" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            全民瘋AI系列 [大語言模型應用與實戰]
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Ollama使用教學
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜尋" placeholder="搜尋" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="搜尋">
        
        <button type="reset" class="md-search__icon md-icon" title="清除" aria-label="清除" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜尋引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/andy6804tw/crazyai-llm" title="前往倉庫" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="導覽列" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="全民瘋AI系列 [大語言模型應用與實戰]" class="md-nav__button md-logo" aria-label="全民瘋AI系列 [大語言模型應用與實戰]" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    全民瘋AI系列 [大語言模型應用與實戰]
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/andy6804tw/crazyai-llm" title="前往倉庫" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    目錄
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    📚李宏毅生成式AI導論(2024)
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            📚李宏毅生成式AI導論(2024)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/1.what-is-generative-ai.md/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.生成式AI是什麼
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/2.why-generative-ai-is-powerful/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.今日的生成式人工智慧厲害在哪裡
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/3.cant-train-ai-train-yourself-part1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.訓練不了人工智慧?你可以訓練你自己(上)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/4.cant-train-ai-train-yourself-part2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4.訓練不了人工智慧?你可以訓練你自己(中)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/5.cant-train-ai-train-yourself-part3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5.訓練不了人工智慧?你可以訓練你自己(下)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/6.llm-history-phase1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.大型語言模型修練史(第一階段)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/7.llm-history-phase2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7.大型語言模型修練史(第二階段)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/8.llm-history-phase3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8.大型語言模型修練史(第三階段)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intro-generative-ai/9.llm-ai-agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9.以大型語言模型打造的 AI Agent
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    📚5-Day Gen AI Intensive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            📚5-Day Gen AI Intensive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-03-30-course-intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    課程介紹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-03-31-day1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    [Day 1]大型語言模型基礎與提示工程
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-04-01-day2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    [Day 2]Embeddings 與向量資料庫
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-04-02-day3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    [Day 3]生成式 AI 代理人
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-04-03-day4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    [Day 4]特定領域大型語言模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../google-genai-course/2025-04-04-day5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    [Day 5] 生成式 AI 的 MLOps
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    💻LLM實戰應用
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            💻LLM實戰應用
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Ollama使用教學
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Ollama使用教學
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目錄">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目錄
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ollama_1" class="md-nav__link">
    <span class="md-ellipsis">
      什麼是 Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_2" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 的核心特色
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      安裝前置作業：系統需求
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_3" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 安裝
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Ollama 安裝">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      方法一 官方下載安裝包 (初學者建議)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker" class="md-nav__link">
    <span class="md-ellipsis">
      方法二 使用 Docker 安裝 (專業玩家使用)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="方法二 使用 Docker 安裝 (專業玩家使用)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-docker" class="md-nav__link">
    <span class="md-ellipsis">
      1. 先安裝 Docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      2. 啟動 Ollama 容器
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 啟動 Ollama 容器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu" class="md-nav__link">
    <span class="md-ellipsis">
      使用 CPU 啟動服務
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      使用 GPU 啟動服務
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_4" class="md-nav__link">
    <span class="md-ellipsis">
      使用 Ollama 下載和運行模型
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rest-api" class="md-nav__link">
    <span class="md-ellipsis">
      REST API
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_5" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 使用技巧
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      總結
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      延伸閱讀
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Ollama%20API%E6%95%B4%E5%90%88OpenAI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ollama API整合OpenAI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E4%BD%BF%E7%94%A8Colab%E9%83%A8%E7%BD%B2Ollama%E6%9C%8D%E5%8B%99/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    用Colab部署Ollama服務
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E4%BD%BF%E7%94%A8Streamlit%E8%88%87Ollama%E5%BB%BA%E7%AB%8B%E8%81%8A%E5%A4%A9%E4%BB%8B%E9%9D%A2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    使用Streamlit與Ollama建立聊天介面
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../openai-agents-sdk-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI Agents SDK多代理人工作流程教學
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mcp-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    深入理解 MCP 搭配簡易實作
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    🤖免費 LLM API 串接資源
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            🤖免費 LLM API 串接資源
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../free-llm-api-integration-resources/github-models-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GitHub Models 免費試玩
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../free-llm-api-integration-resources/groq-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Groq 免費體驗 Llama API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    🛠️熱門AI工具使用教學
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            🛠️熱門AI工具使用教學
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../aigc-tools/AI%E5%9C%96%E7%89%87%E7%94%9F%E6%88%90-Raphael%E5%85%8D%E8%B2%BBAI%E5%9C%96%E5%83%8F%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🖼️Raphael免費AI圖像生成工具
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../aigc-tools/AI%E7%94%9F%E6%88%90%E9%9F%B3%E6%A8%82-%E7%94%A8Suno.ai%E8%BC%95%E9%AC%86%E6%89%93%E9%80%A0%E5%B1%AC%E6%96%BC%E4%BD%A0%E7%9A%84AI%E9%9F%B3%E6%A8%82/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    🎵Suno作曲及音樂生成器
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../aigc-tools/%E5%88%A9%E7%94%A8%20Google%20NotebookLM%20%E6%89%93%E9%80%A0%E9%AB%98%E6%95%88%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    📝利用 Google NotebookLM 打造高效學習筆記
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目錄">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目錄
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ollama_1" class="md-nav__link">
    <span class="md-ellipsis">
      什麼是 Ollama?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_2" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 的核心特色
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      安裝前置作業：系統需求
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_3" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 安裝
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Ollama 安裝">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      方法一 官方下載安裝包 (初學者建議)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker" class="md-nav__link">
    <span class="md-ellipsis">
      方法二 使用 Docker 安裝 (專業玩家使用)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="方法二 使用 Docker 安裝 (專業玩家使用)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-docker" class="md-nav__link">
    <span class="md-ellipsis">
      1. 先安裝 Docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ollama" class="md-nav__link">
    <span class="md-ellipsis">
      2. 啟動 Ollama 容器
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 啟動 Ollama 容器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu" class="md-nav__link">
    <span class="md-ellipsis">
      使用 CPU 啟動服務
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      使用 GPU 啟動服務
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_4" class="md-nav__link">
    <span class="md-ellipsis">
      使用 Ollama 下載和運行模型
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rest-api" class="md-nav__link">
    <span class="md-ellipsis">
      REST API
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ollama_5" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama 使用技巧
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      總結
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      延伸閱讀
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  
<nav class="md-tags" >
  
    
    
    
      <a href="../../tags/#ollama" class="md-tag">Ollama</a>
    
  
</nav>



<h1 id="ollama">Ollama 使用教學：在自己的電腦上運行大型語言模型</h1>
<h2 id="ollama_1">什麼是 Ollama?</h2>
<p>Ollama 並非一個大型語言模型，而是協助你快速安裝、管理與運行各種 LLM 的開源工具。換句話說，它就像一個「語言模型管家」，幫你整合安裝流程、記憶體配置，以及提供 REST API 介面，讓其他程式可以把 Ollama 當成後端 AI 服務來使用。透過 Ollama，你可以更方便地在自己的電腦（或伺服器）上進行模型測試、開發，甚至直接部署應用。</p>
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nHJOSw5GlIPVmenoDmQmWg.png" /></p>
<p>Ollama 的使用情境非常廣泛，尤其適合想離線運作 AI 或在本地端保有數據隱私的使用者。例如，你可以用它來測試不同大小、不同架構的模型，包括小型的 7B 參數模型，或是超大型的 70B 參數模型，依照你的硬體資源調整執行策略。它也支援量化技術，讓模型在一般 RAM 配置中依舊能順暢跑起來。</p>
<h2 id="ollama_2">Ollama 的核心特色</h2>
<p><img alt="" src="../images/img-ollama-intro-1.png" /></p>
<p><strong>1. 支援多種大型語言模型</strong><br />
Ollama 本身並不限定你只能跑 llama 模型，而是彈性地整合多個 LLM。你可以用它來安裝像是 DeepSeek-R1、Llama、Gemma 等常見的模型，透過簡單的命令就能快速切換或更新。</p>
<p><strong>2. 自動偵測硬體資源</strong><br />
無論你的設備是只有 CPU，還是配備了 NVIDIA RTX 30、40 系列等 GPU，Ollama 都能自動偵測並選擇最佳的推理模式。若偵測到 GPU，可大幅加速運行；如果只有 CPU，也能以較慢的速度執行小型或量化後的模型。</p>
<p><strong>3. REST API 介面</strong><br />
Ollama 提供 REST API，讓你可以很容易地在其他程式語言（例如 Python、Node.js、Go 等）中呼叫 Ollama，將其當作後端的 AI 回答服務。你可以把 Ollama 視為一個可本機端部署的「AI 伺服器」。</p>
<p><strong>4. 輕鬆管理模型下載與更新</strong><br />
Ollama 會把各種模型集中管理，包含檔案下載、版本更新等，減少你手動去 GitHub 或其他資源抓模型檔的麻煩。對於需要常常測試新模型或版本的人來說，這點相當方便。</p>
<p><strong>5. 量化技術支援</strong><br />
為了減少記憶體需求，Ollama 支援量化後的模型。以 Llama 2 7B 為例，原本需要超過 14GB 記憶體，在 4-bit 量化後只需要大約 4GB，即使在沒有高階 GPU 的情況下，也能跑起基本推理工作。</p>
<h2 id="_1">安裝前置作業：系統需求</h2>
<p>在正式安裝並執行 Ollama 之前，你需要先確認自己的硬體、作業系統或硬碟是否符合需求。以下整理幾項重點:</p>
<p><strong>1. CPU</strong></p>
<ul>
<li>如果電腦沒有 GPU，Ollama 會使用 CPU 來跑模型推理。  </li>
<li>純 CPU 的推理速度可能較慢，適合測試小型模型或量化後的模型。</li>
</ul>
<p><strong>2. GPU （可選）</strong></p>
<ul>
<li>若電腦/伺服器上有支援 CUDA 的 NVIDIA 顯示卡（如 GeForce RTX 30、40 系列等），Ollama 能自動偵測並啟用 GPU 加速，推理速度將大幅提升。  </li>
<li>具體可支援的 GPU 列表可參考官方說明或 Ollama <a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md">GitHub 頁面</a>。</li>
</ul>
<p><strong>3. 記憶體（RAM）</strong></p>
<ul>
<li>記憶體需求主要取決於模型大小。像 Llama 2 7B 模型推理時，未量化大約需要 14GB，量化後可降至 4GB 左右。  </li>
<li>如果你打算跑 70B 等級的大模型（例如 Llama 3.3 70B，下載包可達 40GB+），至少要準備 32GB 以上的 RAM 才比較保險。此外也要注意硬碟儲存空間的大小。</li>
</ul>
<p><strong>4. 作業系統</strong></p>
<ul>
<li>Linux、macOS、Windows（需要在 WSL 或 Docker 環境）等都能運行 Ollama，但建議以 Linux 或 macOS 為主，環境配置相對完整。  </li>
<li>如果在 Windows 上使用，最好先確認 Docker 或 WSL2 是否裝好。</li>
</ul>
<p><strong>5. 硬碟空間</strong></p>
<ul>
<li>除了安裝 Ollama 本身的空間外，更大宗的佔用是各種模型檔案。例如：  </li>
<li>DeepSeek-R1（7B）：下載大小約 4.7GB  </li>
<li>Llama 3.3（70B）：下載大小約 43GB  </li>
<li>Gemma 2（9B）：下載大小約 5.5GB  </li>
<li>請評估你的硬碟空間，避免安裝大模型時系統爆滿。</li>
</ul>
<p><strong>6. Python 環境 （可選）</strong></p>
<p>建議先在電腦上安裝 Python，以方便後續的應用測試（若只是單純進行 no-code 整合應用，則可省略安裝，並參考下一篇介紹的 Open WebUI）。對開發需求的初學者來說，<strong>Anaconda</strong> 會是相當方便的選擇，因為它能一次安裝好 Python 本身與多種常用的科學運算套件。  </p>
<ul>
<li>可以參考 <a href="https://andy6804tw.github.io/crazyai-python/1.%E7%92%B0%E5%A2%83%E8%A8%AD%E5%AE%9A/1.1%E7%92%B0%E5%AE%89%E8%A3%9D%E6%8C%87%E5%8D%97/">Python 環境安裝指南</a> 來進行環境設定。  </li>
<li>對於有開發需求的人而言，除了 Ollama CLI 工具本身，也可能利用 Python 來呼叫 Ollama 提供的 API，進而做更進階的應用整合。</li>
</ul>
<p>以上列出的硬體與系統需求，是為了確保 Ollama 能順利運行、並最大程度發揮效能。如果你的環境都符合這些要求，接下來就能正式進入安裝與測試的階段囉！</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>以下教學操作的環境以 Linux Server 為主，Windows 或 MacOS 的操作方式基本上差不多！</p>
</div>
<h2 id="ollama_3">Ollama 安裝</h2>
<h3 id="_2">方法一 官方下載安裝包 (初學者建議)</h3>
<p>首先，前往 Ollama <a href="https://ollama.com/">官方網站</a>，在首頁找到「Download」按鈕，選擇適合自己作業系統的安裝方式。</p>
<p><img alt="" src="../images/img-ollama-intro-2.png" /></p>
<p>本文以 Linux 系統作為展示，在終端機輸入以下指令，一鍵下載並安裝 Ollama：</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://ollama.com/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-3.png" /></p>
<p>安裝完成後，Ollama 服務會自動在背景運行。你可以在終端機輸入 <code>ollama -v</code>，若顯示版本資訊或指令列表，即代表安裝成功。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>ollama<span class="w"> </span>-v
</span></code></pre></div>
<p>同時要確認 Ollama server 是否成功被啟動，可以在終端機中輸入以下指令，透過 curl 測試:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>curl<span class="w">  </span>http://localhost:11434
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-4.png" /></p>
<p>亦或是可以在瀏覽器輸入 <code>http://localhost:11434</code> 連線成功將會看到 <code>Ollama is running</code> 的字串訊息。</p>
<p><img alt="" src="../images/img-ollama-intro-5.png" /></p>
<p>若沒有被正常啟動服務，可以手動輸入以下指令啟動 Ollama 服務:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>ollama<span class="w"> </span>serve
</span></code></pre></div>
<details class="note">
<summary>如何讓 Ollama 可供外部存取</summary>
<p>如果你有固定 IP，並希望讓其他電腦可以連線至此台機器，你需要執行以下指令，使 Ollama 監聽所有網路介面：</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="nv">OLLAMA_HOST</span><span class="o">=</span><span class="m">0</span>.0.0.0<span class="w"> </span>ollama<span class="w"> </span>serve
</span></code></pre></div>
<p>或者，你可以修改 <strong>Ollama 的 systemd 服務設定</strong>，讓它在 <code>0.0.0.0:11434</code> 監聽連線，以便外部設備能夠存取。</p>
</details>
<p>若想要終止 Ollama 背景運行，在 Linux 系統可以輸入以下指令停止該程序。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>sudo<span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>ollama
</span></code></pre></div>
<p>在 Linux 系統中設有開機自動運行腳本，因此手動終止程序後可以再透過以下指令啟動。</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>sudo systemctl start ollama
</span></code></pre></div>
<p>如果你在 Windows 上運行 Ollama，可以打開 工作管理員 (Task Manager)，找到 ollama.exe，然後手動結束程式。在 macOS 上，你可以在螢幕右上角的 工具列 找到 Ollama 圖示，點擊後選擇 <code>Quit</code> 來終止程式。</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<ul>
<li>稍後會詳細說明常用的Ollama指令，以及如何下載和測試LLM模型。</li>
<li>在完成方法一安裝並確認 Ollama 已成功在背景運行後，可以直接跳轉至<a href="#ollama_4">使用 Ollama 下載和運行模型</a>來開始使用模型。</li>
</ul>
</div>
<h3 id="docker">方法二 使用 Docker 安裝 (專業玩家使用)</h3>
<p>如果你偏好使用容器化的方式部署 Ollama，也可以透過 Docker 來啟動服務。以下以 Ubuntu 為例，示範啟動方式：</p>
<h4 id="1-docker">1. 先安裝 Docker</h4>
<p>如果系統尚未安裝 Docker，請先安裝。以 Linux Server 系統為例，可先透過以下指令更新並安裝 Docker Engine:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>sudo<span class="w"> </span>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://get.docker.com<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
</span></code></pre></div>
<p>啟動Docker 服務:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>systemctl<span class="w"> </span>start<span class="w"> </span>docker
</span></code></pre></div>
<p>安裝完成後，確認 Docker 版本:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>docker<span class="w"> </span>--version
</span></code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果你的作業系統有GUI介面推薦使用，<a href="https://docs.docker.com/get-started/get-docker/">Docker Desktop</a> 軟體來管理 Docker 容器。</p>
</div>
<h4 id="2-ollama">2. 啟動 Ollama 容器</h4>
<h5 id="cpu">使用 CPU 啟動服務</h5>
<p>如果只使用 CPU 進行推論：</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>-v<span class="w"> </span>ollama:/root/.ollama<span class="w"> </span>-p<span class="w"> </span><span class="m">11434</span>:11434<span class="w"> </span>--name<span class="w"> </span>ollama<span class="w"> </span>ollama/ollama
</span></code></pre></div>
<h5 id="gpu">使用 GPU 啟動服務</h5>
<p>如果需要使用 Nvidia GPU 進行加速推論，必須安裝 NVIDIA Container Toolkit ，讓系統能夠使用 GPU。若不確定自己的 GPU 是否支援，可以先到 <a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md">Ollama GuiHub</a> 官方的支援列表查看。首先確保本機已安裝 Nvidia GPU驅動程式:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>nvidia-smi
</span></code></pre></div>
<p>若成功出現以下畫面，則表示系統已正確配置 GPU 驅動程式。若尚未安裝，您可以參考這篇文章進行設定： <a href="https://andy6804tw.github.io/2025/02/20/linux-install-nvidia-driver/">Linux 安裝 NVIDIA GPU 驅動完整教學</a>。</p>
<p><img alt="" src="../images/img-ollama-intro-6.png" /></p>
<p>接著，在 Linux 安裝 NVIDIA Container Toolkit 使⁠ Docker 容器能使用 GPU 運算環境。基本上按照<a href="https://hub.docker.com/r/ollama/ollama">官方安裝步驟</a>就能完成。首先使用 Apt 安裝，這段指令的主要目的是 設定 NVIDIA Container Toolkit 的 APT 軟體庫 (repository)，以便可以使用 apt 來安裝與更新 NVIDIA Container Toolkit。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://nvidia.github.io/libnvidia-container/gpgkey<span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="w">    </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>gpg<span class="w"> </span>--dearmor<span class="w"> </span>-o<span class="w"> </span>/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>curl<span class="w"> </span>-s<span class="w"> </span>-L<span class="w"> </span>https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list<span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="w">    </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span><span class="s1">&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="w">    </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/apt/sources.list.d/nvidia-container-toolkit.list
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>sudo<span class="w"> </span>apt-get<span class="w"> </span>update
</span></code></pre></div>
<p>設定完成後，即可使用 Apt 安裝 NVIDIA Container Toolkit packages:</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nvidia-container-toolkit
</span></code></pre></div>
<p>將 NVIDIA Container Toolkit 設定為 Docker 的執行時環境，並重新啟動 Docker 服務，使變更生效。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>docker
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</span></code></pre></div>
<p>安裝完成後，就可以在 Docker 或其他容器環境中使用 NVIDIA GPU 加速了。最後就使用 docker 啟動含有 GPU 的 Ollama 服務吧!</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--gpus<span class="o">=</span>all<span class="w"> </span>-v<span class="w"> </span>ollama:/root/.ollama<span class="w"> </span>-p<span class="w"> </span><span class="m">11434</span>:11434<span class="w"> </span>--name<span class="w"> </span>ollama<span class="w"> </span>ollama/ollama
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-7.png" /></p>
<p>部署成功後，我們可以進入正在運行的 Ollama Docker 容器，並開啟一個互動式 Bash Shell，以便直接在容器內操作。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>ollama<span class="w"> </span>bash
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-8.png" /></p>
<p>目前為止已經完成 Ollama 的環境安裝，並且啟動了 Ollama 服務。接下來要講解常用的指令，以及如何管理大型語言模型。</p>
<h2 id="ollama_4">使用 Ollama 下載和運行模型</h2>
<p>Ollama 安裝好之後，在終端機中，執行以下指令下載對應的大語言模型模型(例如: gemma2:9b)：</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>ollama<span class="w"> </span>pull<span class="w"> </span>gemma2:9b
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-9.png" /></p>
<p>在 <a href="https://ollama.com/search">Ollama 官方的 Models</a> 頁面上，提供了多種支援的模型。如果對這些模型較為熟悉，可以根據機器的配置選擇適合的模型，無論是較大的還是較小的版本。</p>
<p><img alt="" src="../images/img-ollama-intro-10.png" /></p>
<p>下載完成後，使用以下指令，透過終端機介面來使用剛剛下載的大型語言模型。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>ollama<span class="w"> </span>run<span class="w"> </span>gemma2:9b
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-11.gif" /></p>
<p>如果輸出如上圖所示內容，則表示大語言模型正常運作。輸入 <code>/bye</code> 或是 <code>ctrl+d</code> 退出問答介面。</p>
<h2 id="rest-api">REST API</h2>
<p>除了直接在終端機使用互動模式，Ollama 預設也提供了一個 REST API，讓你可以透過程式或第三方工具（例如 Open WebUI）來呼叫並管理模型。換句話說，Ollama 不單只是一個聊天介面，而是能被視為一個「大型語言模型服務」：接收使用者送出的 prompt 後，進行推理並產生對應的答案。下面範例示範如何使用 <code>curl</code> 向本地端 Ollama 服務發送請求，指定要使用的模型（這裡是 <code>gemma2:9b</code>）並傳入對話內容：</p>
<p><strong>Completion API(單次問答)</strong></p>
<p>運行服務後，若要透過 API 來生成內容，可使用以下兩種方式。其中第一種方式會一次性返回解答，類似過去 OpenAI API Service 所提供的 Completion API。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>curl<span class="w"> </span>http://localhost:11434/api/generate<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="s1">  &quot;model&quot;: &quot;gemma2:9b&quot;,</span>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="s1">  &quot;system&quot;: &quot;你是一位嚴謹的地理老師，請使用繁體中文回答問題。&quot;,</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="s1">  &quot;prompt&quot;: &quot;請問法國的首都？&quot;,</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a><span class="s1">  &quot;stream&quot;: false</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a><span class="s1">}&#39;</span>
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-12.png" /></p>
<p><strong>Chat Completion API（對話生成）</strong></p>
<p>第二種方法是目前主流的對話生成模式，類似於 OpenAI API Service 所提供的 Chat Completion API。簡單來說，Chat Completion 的核心概念就是「補完對話」：我們先給 AI 一組對話內容作為上下文，再設定特定的條件或角色，接著讓 AI 根據這些情境繼續對話，產生後續回應。這樣一來，整段對話就能不斷延伸與發展。</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>curl<span class="w"> </span>http://localhost:11434/api/chat<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="s1">  &quot;model&quot;: &quot;gemma2:9b&quot;,</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="s1">  &quot;messages&quot;: [</span>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="s1">    { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是一位嚴謹的地理老師，請使用繁體中文回答問題。&quot; },</span>
</span><span id="__span-20-5"><a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="s1">    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;請問美國的首都?&quot; },</span>
</span><span id="__span-20-6"><a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="s1">    { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;美國的首都是 **華盛頓哥倫比亞特區** 。&quot; },</span>
</span><span id="__span-20-7"><a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a><span class="s1">    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;請問法國的首都?&quot; }</span>
</span><span id="__span-20-8"><a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a><span class="s1">  ],</span>
</span><span id="__span-20-9"><a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a><span class="s1">  &quot;stream&quot;: false</span>
</span><span id="__span-20-10"><a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a><span class="s1">}&#39;</span>
</span></code></pre></div>
<p><img alt="" src="../images/img-ollama-intro-13.png" /></p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<ul>
<li>更詳細的 API 參數說明與使用方式，請參考 <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">Ollama 的官方文件</a>。</li>
<li>或是參考下一篇教學文章： <a href="../Ollama%20API%E6%95%B4%E5%90%88OpenAI/">Ollama API整合OpenAI</a></li>
</ul>
</div>
<h2 id="ollama_5">Ollama 使用技巧</h2>
<p>以下整理幾個 Ollama 常用的指令，方便你快速上手並管理大型語言模型。後續若有進階需求，可再深入查看官方文件或使用 <code>ollama help</code> 指令掌握更多參數與用法。</p>
<table>
<thead>
<tr>
<th>指令</th>
<th>用途</th>
<th>範例</th>
<th>補充說明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ollama version</code></td>
<td>檢查目前安裝的 Ollama 版本</td>
<td><code>ollama version</code></td>
<td>確認指令是否正常運作，以及版本是否符合需求</td>
</tr>
<tr>
<td><code>ollama list</code></td>
<td>列出本機已安裝或下載的所有模型</td>
<td><code>ollama list</code></td>
<td>快速查看目前可使用的模型清單</td>
</tr>
<tr>
<td><code>ollama pull [model_name]</code></td>
<td>從遠端下載（或更新）指定模型</td>
<td><code>ollama pull llama2</code></td>
<td>下載過程中若檔案體積較大，需確保網路穩定，下載完後即可使用</td>
</tr>
<tr>
<td><code>ollama run [model_name]</code></td>
<td>執行指定的模型並進入互動式模式</td>
<td><code>ollama run llama2</code></td>
<td>模型載入後，可直接在終端機進行對話或測試</td>
</tr>
<tr>
<td><code>ollama rm [model_name]</code></td>
<td>移除指定的模型</td>
<td><code>ollama rm llama2</code></td>
<td>刪除不再需要的模型，能節省空間；使用前請先確認是否確定要移除</td>
</tr>
<tr>
<td><code>ollama help</code></td>
<td>顯示 Ollama CLI 的所有可用指令與參數</td>
<td><code>ollama help</code></td>
<td>如果忘記用法或想了解進階功能，可用此指令取得官方幫助資訊</td>
</tr>
<tr>
<td>其他常見參數（於 <code>run</code> 中使用）</td>
<td>例如 <code>--temperature</code>、<code>--top_p</code>、<code>--max_tokens</code> 等參數</td>
<td><code>ollama run llama2 --temperature 0.7</code></td>
<td>調整模型生成文字的「創造性」程度、抽樣分佈或輸出長度等，細節可參考官方文件</td>
</tr>
</tbody>
</table>
<p>若需要確認 Ollama 是否有使用到 CPU 或 GPU，也可使用 <code>ollama ps</code> 指令來查看詳細資訊。更多相關說明，請參考官方文件：<a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu">Ollama FAQ</a>。</p>
<p><img alt="" src="../images/img-ollama-intro-14.png" /></p>
<p>我們可以使用 <code>ollama list</code> 來查看本機上已下載的 LLM 模型。這些模型會存放在以下系統資料夾中：</p>
<ul>
<li><strong>macOS</strong>: ~/.ollama/models</li>
<li><strong>Linux</strong>: /usr/share/ollama/.ollama/models</li>
<li><strong>Windows</strong>: C:\Users\%username%.ollama\models</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果你是透過 Docker 運行 Ollama，模型的存放位置會依據 Docker Volume（磁碟掛載） 設定來決定。</p>
</div>
<p><img alt="" src="../images/img-ollama-intro-15.png" /></p>
<details class="note">
<summary>如何找到 Docker Volume 的實體位置?</summary>
<ul>
<li>指令中的 -v ollama:/root/.ollama 會讓模型存放在 Docker Volume 內，而不是主機的 /root/.ollama。</li>
<li>你可以使用 <code>docker volume inspect ollama</code> 來查看實際存放位置。</li>
</ul>
<p><strong>列出 Docker Volumes</strong></p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span>volume<span class="w"> </span>ls
</span></code></pre></div>
<p>你會看到類似的輸出：</p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>DRIVER<span class="w">    </span>VOLUME<span class="w"> </span>NAME
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="nb">local</span><span class="w">     </span>ollama
</span></code></pre></div>
<p>這表示你的 ollama Volume 已經建立。</p>
<p><strong>查找 ollama Volume 的具體存放位置</strong></p>
<div class="language-sh highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span>volume<span class="w"> </span>inspect<span class="w"> </span>ollama
</span></code></pre></div>
<p>其中 <code>Mountpoint</code> 顯示了實體存放位置。</p>
</details>
<h2 id="_3">總結</h2>
<p>在這篇文章中，我們已經介紹了 Ollama 這個強大的大型語言模型（LLM）管理工具，不僅能輕鬆安裝、切換與執行各種模型，還提供了 REST API 介面，方便開發者結合不同程式或服務。如果你覺得純指令操作比較麻煩，或想用更友善的視覺介面跟 AI 模型互動，可以嘗試搭配「Open WebUI」來實現。Open WebUI 的操作介面與 ChatGPT 十分相似，除了能聊天，還能進行 AI 繪圖、圖像辨識、RAG 檢索增強生成，以及整理 PDF 檔案內容或搜尋網頁等進階功能。最棒的是，整套流程不用寫任何一行 Python 程式碼就能上手！</p>
<p>下一篇文章將會進一步示範如何讓 Ollama 與 Open WebUI 整合，讓你在本地端打造功能豐富、使用流暢的 AI 服務。想知道更多細節或尋找更便利的操作方式，就請持續關注下一篇囉！</p>
<h2 id="_4">延伸閱讀</h2>
<ul>
<li><a href="https://www.53ai.com/news/zhinengyingjian/2024103038917.html">教你搭建 Ollama + Gradio 聊天机器人</a></li>
<li><a href="https://blog.miniasp.com/post/2024/04/17/Run-TAIDE-LX-7B-Chat-4bit-model-in-Ollama">使用 Ollama 執行 TAIDE 的 TAIDE-LX-7B-Chat-4bit 大語言模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21161140014">VLLM 與Ollama：如何選擇適合的輕量級LLM 框架？</a></li>
<li><a href="https://medium.com/@cch.chichieh/deepseek-本地部屬-llama-cpp-與-ollama-78f24809604f">llama.cpp 與 ollama</a></li>
<li><a href="https://blog.csdn.net/weixin_42458975/article/details/139465823">區分LLaMA、llama.cpp和Ollama</a></li>
<li><a href="https://hub.docker.com/r/ollama/ollama">Ollama Docker image</a></li>
<li><a href="https://ywctech.net/ml-ai/ollama-first-try/#%E6%80%8E%E9%BA%BC%E9%97%9C%E6%8E%89-ollama">五分鐘上手 Ollama - 在本機跑 LLM 語言模型</a></li>
<li><a href="https://ithelp.ithome.com.tw/articles/10357750">講到TIDE 不想讓資料外洩怎麼辦：在本地用 Ollama 搭配 Open WebUI 做一個聊天界面吧</a></li>
<li><a href="https://ivonblog.com/posts/ollama-llm/">這篇提到如何測試影像模型</a></li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright © 2024 - 2025 10程式中
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u8907\u88fd", "clipboard.copy": "\u8907\u88fd", "search.result.more.one": "\u6b64\u9801\u5c1a\u6709 1 \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.more.other": "\u6b64\u9801\u5c1a\u6709 # \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.none": "\u6c92\u6709\u7b26\u5408\u7684\u9805\u76ee", "search.result.one": "\u627e\u5230 1 \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.other": "\u627e\u5230 # \u500b\u7b26\u5408\u7684\u9805\u76ee", "search.result.placeholder": "\u6253\u5b57\u9032\u884c\u641c\u5c0b", "search.result.term.missing": "\u7f3a\u5c11\u5b57\u8a5e", "select.version": "\u9078\u64c7\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="../../javascripts/analytics.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>