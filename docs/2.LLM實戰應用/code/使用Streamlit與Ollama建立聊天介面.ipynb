{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 Streamlit 與 Ollama 建立聊天介面\n",
        "## 在 Google Colab 運行 Streamlit 並讓本機存取\n",
        "Google Colab 預設**無法直接執行 `streamlit run`**，因為 Colab 是一個 **雲端環境**，但我們可以透過 **localtunnel** 或 **ngrok** 來建立一個**可公開存取的網址**，讓本機能夠連上 Colab 內運行的 Streamlit 應用。\n",
        "\n",
        "### **步驟 1：安裝必要套件**\n",
        "在 **Colab** 內執行安裝 streamlit："
      ],
      "metadata": {
        "id": "6TSGiHPd49_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZXNYm9q55CX",
        "outputId": "cc457005-878f-40dc-be55-98f0cf168b4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **程式簡介**\n",
        "這支程式使用 **Streamlit** 建立一個簡單的 **單輪聊天介面**，並透過 **Ollama（類 OpenAI API）** 來處理對話請求。它的主要功能是：\n",
        "1. **使用者輸入訊息**，顯示在聊天框內。\n",
        "2. **程式將訊息傳送至 Ollama API（本機端 11434 連接埠）** 來獲取 AI 回應。\n",
        "3. **顯示 AI 回應**，並將對話內容保留在 `st.session_state` 中，讓使用者可以看到完整的對話紀錄。\n",
        "\n",
        "這是一個簡單的 **本機 AI 助手**，可以用來測試 **Ollama 所管理模型**，或作為更進階應用的基礎。這裡使用 **魔術指令** 在 Colab 中寫入程式：\n"
      ],
      "metadata": {
        "id": "dW9pP4Lq5rjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9RtRjSK4fLF"
      },
      "outputs": [],
      "source": [
        "%%writefile basic.py\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import streamlit as st\n",
        "\n",
        "# 設定 OpenAI API 金鑰與模型端點\n",
        "token = \"ollama\"\n",
        "endpoint = \"http://localhost:11434/v1/\"\n",
        "model_name = \"gemma2:9b\"\n",
        "\n",
        "client = OpenAI(base_url=endpoint, api_key=token)\n",
        "\n",
        "# 單輪對話初始化的 system prompt\n",
        "SYSTEM_PROMPT = {\"role\": \"system\", \"content\": \"你是一個有用的助手。且會根據使用者輸入的語言做回應。\"}\n",
        "\n",
        "# 初始化對話歷史，僅用於顯示（不保存上下文）\n",
        "if \"display_messages\" not in st.session_state:\n",
        "    st.session_state.display_messages = []\n",
        "\n",
        "# 顯示對話歷史\n",
        "for message in st.session_state.display_messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# 接收使用者輸入\n",
        "if user_input := st.chat_input(\"請輸入您的訊息...\"):\n",
        "    # 顯示使用者訊息\n",
        "    st.session_state.display_messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    # 組合單輪對話訊息\n",
        "    messages = [SYSTEM_PROMPT, {\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    # 調用 OpenAI API 獲取回應\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=1.0,\n",
        "        top_p=1.0,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    # 獲取助理回應內容\n",
        "    assistant_response = response.choices[0].message.content\n",
        "\n",
        "    # 顯示助理回應\n",
        "    st.session_state.display_messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(assistant_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 在 Colab 運行程式\n",
        "**指令說明**\n",
        "\n",
        "- `!streamlit run app.py &>/content/logs.txt &`\n",
        "  - 在背景執行 Streamlit，並將輸出結果記錄到 logs.txt\n",
        "- `npx localtunnel --port 8501`\n",
        "  - 使用 LocalTunnel 產生公開網址，讓你的 本機電腦可以存取\n",
        "- `curl ipv4.icanhazip.com`\n",
        "  - 顯示 Colab 機器的外部 IP，用於 localtunnel 網頁驗證"
      ],
      "metadata": {
        "id": "7uVj7y3_Fs91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run basic.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "z8OH5Z3pFtlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "此命令會運行 **Streamlit 應用程式 (`basic.py`)**，並將輸出記錄到 `logs.txt`。同時，它使用 **localtunnel** 將應用程式在 **8501 連接埠** 上 **臨時公開到網際網路**。\n",
        "\n",
        "在 **Colab** 執行該命令後，應該會顯示類似以下的輸出內容：\n",
        "\n",
        "請**複製 IP 位址**（例如 **35.185.117.70**），然後**點擊產生的 URL**，即可進入應用程式的頁面。\n"
      ],
      "metadata": {
        "id": "HAwlw9AnFxuv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMkaP-ZkFyQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}